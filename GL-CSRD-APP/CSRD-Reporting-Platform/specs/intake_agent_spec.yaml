# ============================================================================
# AGENT SPECIFICATION: ESG DATA INTAKE & VALIDATION AGENT
# AgentSpec V2.0 Compliant
# ============================================================================

# ----------------------------------------------------------------------------
# SECTION 1: AGENT METADATA
# ----------------------------------------------------------------------------
agent_metadata:
  agent_id: "csrd_intake_agent"
  agent_name: "IntakeAgent"
  display_name: "ESG Data Intake & Validation Agent"
  version: "1.0.0"
  domain: "CSRD/ESG Reporting"
  subdomain: "Data Ingestion & Validation"
  agent_type: "data-processor"
  complexity: "medium"
  priority: "high"
  status: "production"
  deterministic: true
  llm_usage: false
  zero_hallucination: true

# ----------------------------------------------------------------------------
# SECTION 2: DESCRIPTION
# ----------------------------------------------------------------------------
description:
  purpose: |
    Ingest and validate ESG data from multiple sources, ensuring data quality
    and compliance with ESRS data point specifications.

  strategic_context:
    global_impact: "Supports CSRD compliance for 50,000+ EU companies"
    opportunity: "Automate ESG data validation reducing manual effort by 80%"
    market_size: "EU CSRD compliance market estimated at €5B annually"
    technology_maturity: "TRL 8 (Proven technology, production-ready)"

  capabilities:
    - "Multi-format data ingestion (CSV, JSON, Excel, Parquet, API)"
    - "Schema validation against ESRS data point catalog (1,082 data points)"
    - "Data quality assessment (completeness, accuracy, consistency)"
    - "Automated field mapping to ESRS taxonomy"
    - "Statistical outlier detection and flagging"
    - "Data transformation and normalization"
    - "Complete audit trail generation"

  key_features:
    - "1,000+ records/sec throughput"
    - "100% deterministic processing"
    - "Zero hallucination guarantee (no AI estimation)"
    - "Complete audit trail"
    - "Automated error reporting"

  dependencies:
    - agent_id: "materiality_agent"
      relationship: "provides_data_to"
      data: "validated_esg_data"
    - agent_id: "calculator_agent"
      relationship: "provides_data_to"
      data: "validated_esg_data"

# ----------------------------------------------------------------------------
# INPUTS
# ----------------------------------------------------------------------------

inputs:
  primary_inputs:
    - name: "esg_data"
      type: "file"
      formats: ["csv", "json", "excel", "parquet"]
      required: true
      schema: "schemas/esg_data.schema.json"
      description: "Raw ESG data from company systems"
      validation:
        - "File size: <500MB"
        - "Row count: <100,000 rows"
        - "Required columns: metric_code, value, unit"

    - name: "company_profile"
      type: "json"
      required: true
      schema: "schemas/company_profile.schema.json"
      description: "Company metadata and context"

  reference_data:
    - name: "esrs_data_points"
      path: "data/esrs_data_points.json"
      description: "ESRS data point catalog (1,082 points)"
      required: true

    - name: "data_quality_rules"
      path: "rules/data_quality_rules.yaml"
      description: "Data quality validation rules"
      required: true

# ----------------------------------------------------------------------------
# OUTPUTS
# ----------------------------------------------------------------------------

outputs:
  - name: "validated_esg_data"
    format: "json"
    description: "Validated and enriched ESG data"
    structure:
      metadata:
        total_records: "integer"
        valid_records: "integer"
        invalid_records: "integer"
        warnings: "integer"
        data_quality_score: "float (0-100)"
        processing_time_seconds: "float"
      data_points: "array of validated ESG data points"
      errors: "array of validation errors"
      warnings: "array of data quality warnings"

  - name: "data_quality_report"
    format: "json"
    description: "Comprehensive data quality assessment"
    optional: false

# ----------------------------------------------------------------------------
# PROCESSING WORKFLOW
# ----------------------------------------------------------------------------

processing_workflow:
  step_1_data_ingestion:
    name: "Data Ingestion"
    description: "Read data from input files"
    tasks:
      - "Detect file format"
      - "Parse file content"
      - "Load data into memory/dataframe"
      - "Count total records"
    error_handling: "Fail if file cannot be parsed"

  step_2_schema_validation:
    name: "Schema Validation"
    description: "Validate against JSON schema"
    tasks:
      - "Load esg_data.schema.json"
      - "Validate each record against schema"
      - "Collect schema validation errors"
    validations:
      - "Required fields present"
      - "Data types correct"
      - "Field formats valid (dates, codes, etc.)"
    error_handling: "Flag invalid records, continue processing"

  step_3_esrs_mapping:
    name: "ESRS Taxonomy Mapping"
    description: "Map data to ESRS data points"
    tasks:
      - "Load ESRS data points catalog"
      - "Match metric_code to ESRS codes"
      - "Enrich with ESRS metadata (standard, disclosure, unit)"
      - "Flag unmapped metrics"
    mapping_logic: |
      IF exact_match(metric_code, esrs_code):
        esrs_data_point = esrs_catalog[esrs_code]
      ELSE IF fuzzy_match(metric_name, esrs_name, threshold=0.9):
        esrs_data_point = esrs_catalog[matched_code]
        add_warning("Fuzzy match used")
      ELSE:
        add_warning("No ESRS mapping found")

  step_4_data_quality_checks:
    name: "Data Quality Assessment"
    description: "Run all data quality rules"
    tasks:
      - "Load data quality rules"
      - "Execute completeness checks"
      - "Execute accuracy checks"
      - "Execute consistency checks"
      - "Execute validity checks"
      - "Calculate quality score"
    quality_dimensions:
      completeness:
        checks: ["mandatory fields", "missing values", "time series gaps"]
        weight: 0.30
      accuracy:
        checks: ["value ranges", "outliers", "YoY changes"]
        weight: 0.25
      consistency:
        checks: ["internal calculations", "unit consistency", "cross-metric"]
        weight: 0.20
      timeliness:
        checks: ["data currency", "reporting period"]
        weight: 0.15
      validity:
        checks: ["data types", "enums", "formats"]
        weight: 0.10

  step_5_outlier_detection:
    name: "Statistical Outlier Detection"
    description: "Identify anomalous values"
    methods:
      - name: "Z-score method"
        threshold: 3.0
        description: "Flag values >3 standard deviations from mean"
      - name: "IQR method"
        description: "Flag values outside 1.5×IQR from quartiles"
      - name: "YoY comparison"
        threshold: 0.50
        description: "Flag >50% year-over-year changes"
    action: "Flag outliers as warnings, do not reject"

  step_6_data_enrichment:
    name: "Data Enrichment"
    description: "Add metadata and provenance"
    enrichments:
      - "Add ESRS standard"
      - "Add disclosure requirement"
      - "Add expected unit"
      - "Add data quality score"
      - "Add processing timestamp"
      - "Add source traceability"

  step_7_output_generation:
    name: "Generate Output"
    description: "Create validated dataset and reports"
    outputs:
      - "validated_esg_data.json"
      - "data_quality_report.json"
      - "error_log.json"

# ----------------------------------------------------------------------------
# SECTION 3: TOOLS
# ----------------------------------------------------------------------------
tools:
  tools_list:
    - tool_id: "schema_validator"
      name: "schema-validator"
      deterministic: true
      category: "validation"
      description: "JSON Schema validation using jsonschema library"

      parameters:
        type: "object"
        properties:
          data: {type: "object", description: "Data to validate"}
          schema: {type: "object", description: "JSON schema definition"}
        required: ["data", "schema"]

      returns:
        type: "object"
        properties:
          valid: {type: "boolean", description: "Validation result"}
          errors: {type: "array", description: "List of validation errors"}

      implementation:
        method: "JSON Schema validation"
        library: "jsonschema>=4.0"
        calculation_method: "Standard JSON Schema Draft 7 validation"
        accuracy: "100% schema compliance"
        validation: "Unit tests with known valid/invalid schemas"
        standards: ["JSON Schema Draft 7", "ESRS Data Point Catalog"]

    - tool_id: "esrs_taxonomy_mapper"
      name: "esrs-taxonomy-mapper"
      deterministic: true
      category: "lookup"
      description: "Database lookup to map fields to ESRS codes"

      parameters:
        type: "object"
        properties:
          metric_code: {type: "string", description: "Metric code to map"}
          metric_name: {type: "string", description: "Metric name for fuzzy matching"}
        required: ["metric_code"]

      returns:
        type: "object"
        properties:
          esrs_code: {type: "string", description: "ESRS data point code"}
          esrs_standard: {type: "string", description: "ESRS standard (E1, S1, etc.)"}
          match_type: {type: "string", description: "exact|fuzzy|none"}

      implementation:
        method: "Dictionary lookup with fuzzy matching fallback"
        database: "data/esrs_data_points.json"
        calculation_method: "Direct hash lookup or fuzzy string matching (ratio >0.9)"
        accuracy: "100% for exact matches, 90%+ for fuzzy matches"
        validation: "Tested against ESRS official catalog"
        standards: ["ESRS Set 1", "ESRS Digital Taxonomy"]

    - tool_id: "data_quality_assessor"
      name: "data-quality-assessor"
      deterministic: true
      category: "analysis"
      description: "Execute data quality rules"

      parameters:
        type: "object"
        properties:
          data: {type: "array", description: "ESG data points"}
          rules: {type: "array", description: "Data quality rules"}
        required: ["data", "rules"]

      returns:
        type: "object"
        properties:
          quality_score: {type: "number", description: "Overall quality score (0-100)"}
          dimension_scores: {type: "object", description: "Scores by dimension"}
          issues: {type: "array", description: "Quality issues found"}

      implementation:
        method: "Rule-based quality assessment"
        rules_file: "rules/data_quality_rules.yaml"
        calculation_method: "Weighted average across 5 quality dimensions"
        accuracy: "100% deterministic rule execution"
        validation: "Tested with known quality scenarios"
        standards: ["ISO 8000 (Data Quality)", "DAMA DMBOK"]

    - tool_id: "outlier_detector"
      name: "outlier-detector"
      deterministic: true
      category: "analysis"
      description: "Statistical outlier detection"

      parameters:
        type: "object"
        properties:
          values: {type: "array", description: "Numeric values to analyze"}
          method: {type: "string", description: "zscore|iqr|yoy_comparison"}
          threshold: {type: "number", description: "Outlier threshold"}
        required: ["values", "method"]

      returns:
        type: "object"
        properties:
          outliers: {type: "array", description: "Outlier values and indices"}
          statistics: {type: "object", description: "Statistical summary"}

      implementation:
        method: "Statistical outlier detection"
        calculation_method: "Z-score (>3σ), IQR (>1.5×IQR), or YoY (>50% change)"
        accuracy: "Standard statistical methods"
        validation: "Tested with synthetic outlier datasets"
        standards: ["Statistical Best Practices"]

# ----------------------------------------------------------------------------
# SECTION 4: AI INTEGRATION
# ----------------------------------------------------------------------------
ai_integration:
  enabled: false
  model: "N/A - Deterministic agent"
  temperature: 0.0
  seed: 42
  provenance_tracking: true
  tool_choice: "N/A"
  max_iterations: 0
  budget_usd: 0.0
  rationale: "This is a deterministic data processing agent with zero AI/LLM usage"

# ----------------------------------------------------------------------------
# SECTION 5: SUB-AGENTS
# ----------------------------------------------------------------------------
sub_agents:
  enabled: false
  coordination_pattern: "N/A"
  sub_agent_list: []
  rationale: "This is a leaf-level agent with no sub-agents"

# ----------------------------------------------------------------------------
# PERFORMANCE TARGETS
# ----------------------------------------------------------------------------

performance:
  throughput: "1,000+ records/sec"
  latency_p99: "100 ms per record"
  memory_footprint: "<2 GB for 50K data points"
  scalability: "Linear scaling up to 100K records"

# ----------------------------------------------------------------------------
# QUALITY GUARANTEES
# ----------------------------------------------------------------------------

guarantees:
  deterministic: true
  zero_hallucination: true
  reproducible: true
  audit_trail: "complete"
  error_handling: "All errors logged and reported"

# ----------------------------------------------------------------------------
# ERROR HANDLING
# ----------------------------------------------------------------------------

error_handling:
  schema_validation_errors:
    severity: "major"
    action: "Flag record, continue processing"
    reporting: "Include in error_log.json"

  unmapped_metrics:
    severity: "warning"
    action: "Flag record, include in output with warning"
    reporting: "Include in warnings section"

  outliers:
    severity: "warning"
    action: "Flag for review, include in output"
    reporting: "Include in data_quality_report"

  file_parsing_errors:
    severity: "critical"
    action: "Halt processing, return error"
    reporting: "Return error message to user"

# ----------------------------------------------------------------------------
# SECTION 6: TESTING
# ----------------------------------------------------------------------------
testing:
  test_coverage_target: 0.80

  test_categories:
    - category: "unit_tests"
      description: "Test individual tool implementations (schema validator, mapper, etc.)"
      count: 15
      examples:
        - "test_schema_validator_valid_input"
        - "test_schema_validator_invalid_input"
        - "test_esrs_mapper_exact_match"
        - "test_esrs_mapper_fuzzy_match"
        - "test_outlier_detector_zscore"

    - category: "integration_tests"
      description: "Test full workflow with various data sources"
      count: 8
      examples:
        - "test_csv_ingestion_workflow"
        - "test_json_ingestion_workflow"
        - "test_excel_ingestion_workflow"
        - "test_parquet_ingestion_workflow"

    - category: "determinism_tests"
      description: "Verify reproducibility of data processing"
      count: 5
      examples:
        - "test_same_input_same_output_100_runs"
        - "test_data_quality_score_deterministic"
        - "test_outlier_detection_reproducible"

    - category: "boundary_tests"
      description: "Test edge cases and error handling"
      count: 10
      examples:
        - "test_empty_input_file"
        - "test_missing_mandatory_fields"
        - "test_invalid_data_types"
        - "test_large_dataset_50k_records"
        - "test_malformed_json"
        - "test_invalid_esrs_codes"

  performance_requirements:
    max_latency_ms: 100000
    max_cost_usd: 0.0
    accuracy_target: 1.0
    throughput: "1000 records/sec"

  test_scenarios:
    - name: "Valid CSV Input"
      input: "examples/demo_esg_data.csv"
      expected_output: "All records validated successfully"

    - name: "Missing Mandatory Fields"
      input: "test_data/missing_fields.csv"
      expected_errors: ["Missing required field: metric_code"]

    - name: "Invalid Data Types"
      input: "test_data/invalid_types.json"
      expected_errors: ["Type error: value must be number"]

    - name: "Outlier Detection"
      input: "test_data/with_outliers.csv"
      expected_warnings: ["Outlier detected: E1-1 = 999999"]

    - name: "Large Dataset (50K records)"
      input: "test_data/large_dataset.parquet"
      expected_performance: "<60 seconds total processing"

# ----------------------------------------------------------------------------
# SECTION 7: DEPLOYMENT
# ----------------------------------------------------------------------------
deployment:
  pack_id: "csrd/intake_agent"
  pack_version: "1.0.0"

  resource_requirements:
    memory_mb: 2048
    cpu_cores: 2
    gpu_required: false
    storage_mb: 500

  dependencies:
    python_packages:
      - "pandas>=2.0,<3.0"
      - "jsonschema>=4.0,<5.0"
      - "numpy>=1.24,<2.0"
      - "pyarrow>=12.0,<13.0"
      - "openpyxl>=3.0,<4.0"
      - "pydantic>=2.0,<3.0"

    greenlang_modules:
      - "greenlang.agents.base"
      - "greenlang.core.schemas"
      - "greenlang.core.validation"

  api_endpoints:
    - endpoint: "/api/v1/csrd/intake/execute"
      method: "POST"
      authentication: "required"
      rate_limit: "100 req/min"

  environment_config:
    - name: "ESRS_DATA_POINTS_PATH"
      required: true
      description: "Path to ESRS data points catalog"
    - name: "DATA_QUALITY_RULES_PATH"
      required: true
      description: "Path to data quality rules file"

# ----------------------------------------------------------------------------
# SECTION 8: DOCUMENTATION
# ----------------------------------------------------------------------------
documentation:
  readme_path: "docs/agents/intake_agent/README.md"
  api_docs_path: "docs/agents/intake_agent/API.md"

  example_use_cases:
    - title: "Manufacturing Company ESG Data Validation"
      description: "Validate annual ESG data from CSV export"
      input_example:
        esg_data: "data/manufacturing_esg_2024.csv"
        company_profile: {"name": "ACME Manufacturing", "sector": "Industrial"}
      output_example:
        total_records: 150
        valid_records: 145
        invalid_records: 5
        data_quality_score: 92.5
      output_summary: "145/150 records validated with 92.5% quality score"

    - title: "Financial Services Multi-Source Ingestion"
      description: "Ingest ESG data from multiple file formats"
      input_example:
        esg_data: ["energy_data.csv", "emissions_data.json", "social_data.xlsx"]
      output_summary: "Multi-format ingestion with unified validation"

    - title: "Large Dataset Processing (50K records)"
      description: "Process large annual sustainability dataset"
      input_example:
        esg_data: "data/large_dataset.parquet"
      output_summary: "50,000 records processed in <60 seconds"

  guides:
    - "Getting Started with IntakeAgent"
    - "Configuring Data Quality Rules"
    - "ESRS Taxonomy Mapping Guide"
    - "Handling Data Quality Issues"

# ----------------------------------------------------------------------------
# SECTION 9: COMPLIANCE
# ----------------------------------------------------------------------------
compliance:
  zero_secrets: true

  standards:
    - "ESRS Set 1 (E1-E5, S1-S4, G1)"
    - "CSRD Directive (EU 2022/2464)"
    - "ISO 8000 (Data Quality)"
    - "DAMA DMBOK (Data Management)"
    - "JSON Schema Draft 7"

  security:
    secret_scanning: "GL-SecScan validated"
    vulnerability_scanning: "pip-audit clean"
    sbom_generated: true
    code_signing: false

  data_privacy:
    pii_handling: "No PII processed"
    data_retention: "As per company policy"
    encryption: "Data in transit: TLS 1.3"

# ----------------------------------------------------------------------------
# SECTION 10: METADATA (Version Control & Changelog)
# ----------------------------------------------------------------------------
metadata:
  created_date: "2025-10-18"
  last_modified: "2025-10-18"
  review_status: "Upgraded to AgentSpec V2.0"

  authors:
    - name: "CSRD Platform Team"
      role: "Development"

  reviewers:
    - name: "Head of AI & Climate Intelligence"
      role: "Technical Review"
      status: "Pending"

  change_log:
    - version: "1.0.0"
      date: "2025-10-18"
      changes: "Initial production release with AgentSpec V2.0 compliance"
      author: "CSRD Platform Team"
      breaking_changes: false
