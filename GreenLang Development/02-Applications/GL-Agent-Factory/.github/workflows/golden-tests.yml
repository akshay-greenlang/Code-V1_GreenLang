# GreenLang Agent Factory - Golden Tests Pipeline
# Runs daily to verify agent determinism and correctness
# Compares outputs against known-good baseline results

name: Golden Tests

on:
  schedule:
    # Run daily at midnight UTC
    - cron: '0 0 * * *'
  workflow_dispatch:
    inputs:
      agent-filter:
        description: 'Agent ID filter (e.g., GL-001, or "all")'
        required: false
        default: 'all'
        type: string
      update-baseline:
        description: 'Update baseline results'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: "3.11"
  GOLDEN_TESTS_DIR: "tests/golden"
  BASELINE_DIR: "tests/golden/baselines"

jobs:
  # =============================================================================
  # Prepare Golden Tests
  # =============================================================================
  prepare:
    name: Prepare Golden Test Suite
    runs-on: ubuntu-latest
    outputs:
      test-matrix: ${{ steps.matrix.outputs.tests }}
      has-tests: ${{ steps.matrix.outputs.has-tests }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Generate test matrix
        id: matrix
        run: |
          # Find all golden test files
          GOLDEN_TESTS=$(find . -path "*/${{ env.GOLDEN_TESTS_DIR }}/*" -name "test_*.py" -o -path "*/${{ env.GOLDEN_TESTS_DIR }}/*" -name "golden_*.py" 2>/dev/null || echo "")

          if [ -z "$GOLDEN_TESTS" ]; then
            # Create default test matrix
            echo 'tests=["core", "agents", "calculations"]' >> $GITHUB_OUTPUT
            echo "has-tests=true" >> $GITHUB_OUTPUT
          else
            TESTS=$(echo "$GOLDEN_TESTS" | \
              sed 's|.*/||' | \
              sed 's|\.py$||' | \
              jq -R -s -c 'split("\n") | map(select(length > 0))')
            echo "tests=$TESTS" >> $GITHUB_OUTPUT
            echo "has-tests=true" >> $GITHUB_OUTPUT
          fi

      - name: Display test matrix
        run: |
          echo "## Golden Test Matrix" >> $GITHUB_STEP_SUMMARY
          echo "Tests to run: ${{ steps.matrix.outputs.tests }}" >> $GITHUB_STEP_SUMMARY

  # =============================================================================
  # Run Golden Tests
  # =============================================================================
  golden-tests:
    name: Golden Tests
    runs-on: ubuntu-latest
    needs: prepare
    if: needs.prepare.outputs.has-tests == 'true'
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
        os: [ubuntu-latest, windows-latest, macos-latest]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-json-report deepdiff
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi
        shell: bash

      - name: Run golden tests
        id: golden
        run: |
          # Create results directory
          mkdir -p golden-results

          # Run golden tests with JSON output
          python -m pytest ${{ env.GOLDEN_TESTS_DIR }}/ \
            --json-report \
            --json-report-file=golden-results/results-${{ matrix.python-version }}-${{ matrix.os }}.json \
            -v \
            --tb=short \
            -x \
            || true
        shell: bash
        continue-on-error: true

      - name: Check determinism across runs
        run: |
          echo "Running determinism check (10 iterations)..."

          python << 'EOF'
          import json
          import subprocess
          import sys
          from collections import defaultdict

          results = defaultdict(list)

          for i in range(10):
              print(f"Run {i+1}/10...")
              result = subprocess.run(
                  ["python", "-m", "pytest", "${{ env.GOLDEN_TESTS_DIR }}/", "-v", "--tb=no", "-q"],
                  capture_output=True,
                  text=True
              )
              results[i] = result.stdout

          # Check for variations
          first_result = results[0]
          variations = []
          for i, result in results.items():
              if result != first_result:
                  variations.append(i)

          if variations:
              print(f"WARNING: Results varied in runs: {variations}")
              sys.exit(1)
          else:
              print("PASS: All 10 runs produced identical results")
          EOF
        shell: bash
        continue-on-error: true

      - name: Compare against baseline
        run: |
          python << 'EOF'
          import json
          import os
          from pathlib import Path

          baseline_dir = Path("${{ env.BASELINE_DIR }}")
          results_file = Path("golden-results/results-${{ matrix.python-version }}-${{ matrix.os }}.json")

          if not results_file.exists():
              print("No results file found, skipping baseline comparison")
              exit(0)

          with open(results_file) as f:
              current_results = json.load(f)

          baseline_file = baseline_dir / f"baseline-${{ matrix.python-version }}.json"

          if not baseline_file.exists():
              print(f"No baseline found at {baseline_file}, creating new baseline...")
              baseline_dir.mkdir(parents=True, exist_ok=True)
              with open(baseline_file, 'w') as f:
                  json.dump(current_results, f, indent=2)
              print("Baseline created")
              exit(0)

          with open(baseline_file) as f:
              baseline = json.load(f)

          # Compare test counts
          current_tests = set(t.get('nodeid', '') for t in current_results.get('tests', []))
          baseline_tests = set(t.get('nodeid', '') for t in baseline.get('tests', []))

          added = current_tests - baseline_tests
          removed = baseline_tests - current_tests

          if added:
              print(f"New tests added: {added}")
          if removed:
              print(f"WARNING: Tests removed: {removed}")

          # Compare outcomes
          current_outcomes = {t.get('nodeid'): t.get('outcome') for t in current_results.get('tests', [])}
          baseline_outcomes = {t.get('nodeid'): t.get('outcome') for t in baseline.get('tests', [])}

          regressions = []
          for test, outcome in current_outcomes.items():
              if test in baseline_outcomes and baseline_outcomes[test] == 'passed' and outcome != 'passed':
                  regressions.append(test)

          if regressions:
              print(f"FAIL: Regressions detected: {regressions}")
              exit(1)

          print("PASS: No regressions detected")
          EOF
        shell: bash

      - name: Upload golden test results
        uses: actions/upload-artifact@v4
        with:
          name: golden-results-${{ matrix.python-version }}-${{ matrix.os }}
          path: golden-results/
          retention-days: 30

  # =============================================================================
  # Cross-Platform Consistency Check
  # =============================================================================
  cross-platform-check:
    name: Cross-Platform Consistency
    runs-on: ubuntu-latest
    needs: golden-tests
    steps:
      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          pattern: golden-results-*
          merge-multiple: true
          path: all-results

      - name: Check cross-platform consistency
        run: |
          python << 'EOF'
          import json
          import os
          from pathlib import Path
          from collections import defaultdict

          results_dir = Path("all-results")
          if not results_dir.exists():
              print("No results directory found")
              exit(0)

          all_results = {}
          for result_file in results_dir.glob("results-*.json"):
              with open(result_file) as f:
                  all_results[result_file.stem] = json.load(f)

          if len(all_results) < 2:
              print("Not enough results for cross-platform comparison")
              exit(0)

          # Extract test outcomes
          outcomes_by_test = defaultdict(dict)
          for platform, results in all_results.items():
              for test in results.get('tests', []):
                  test_id = test.get('nodeid', '')
                  outcome = test.get('outcome', '')
                  outcomes_by_test[test_id][platform] = outcome

          # Check for inconsistencies
          inconsistencies = []
          for test_id, outcomes in outcomes_by_test.items():
              unique_outcomes = set(outcomes.values())
              if len(unique_outcomes) > 1:
                  inconsistencies.append({
                      'test': test_id,
                      'outcomes': outcomes
                  })

          if inconsistencies:
              print("WARNING: Cross-platform inconsistencies detected:")
              for item in inconsistencies:
                  print(f"  - {item['test']}: {item['outcomes']}")
          else:
              print("PASS: All tests produce consistent results across platforms")
          EOF

  # =============================================================================
  # Numerical Precision Check
  # =============================================================================
  precision-check:
    name: Numerical Precision Check
    runs-on: ubuntu-latest
    needs: prepare
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install numpy decimal

      - name: Run precision tests
        run: |
          python << 'EOF'
          import decimal
          import json
          from pathlib import Path

          # Set high precision for decimal operations
          decimal.getcontext().prec = 50

          print("Running numerical precision tests...")

          # Test floating point consistency
          test_cases = [
              (0.1 + 0.2, 0.3, "0.1 + 0.2 == 0.3"),
              (1.0 / 3.0 * 3.0, 1.0, "1/3 * 3 == 1"),
              (10 ** 0.5 ** 2, 10.0, "sqrt(10)^2 == 10"),
          ]

          issues = []
          for actual, expected, description in test_cases:
              if abs(actual - expected) > 1e-15:
                  issues.append(f"{description}: got {actual}, expected {expected}")

          if issues:
              print("WARNING: Floating point precision issues:")
              for issue in issues:
                  print(f"  - {issue}")
          else:
              print("PASS: All precision tests passed")

          # Test decimal precision for financial calculations
          d_test_cases = [
              (decimal.Decimal('0.1') + decimal.Decimal('0.2'), decimal.Decimal('0.3')),
              (decimal.Decimal('100.00') * decimal.Decimal('0.15'), decimal.Decimal('15.00')),
          ]

          for actual, expected in d_test_cases:
              if actual != expected:
                  print(f"FAIL: Decimal precision issue: {actual} != {expected}")
              else:
                  print(f"PASS: {actual} == {expected}")
          EOF

  # =============================================================================
  # Update Baseline (Manual Trigger Only)
  # =============================================================================
  update-baseline:
    name: Update Baseline
    runs-on: ubuntu-latest
    needs: [golden-tests, cross-platform-check]
    if: inputs.update-baseline == true
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download results
        uses: actions/download-artifact@v4
        with:
          pattern: golden-results-*
          merge-multiple: true
          path: golden-results

      - name: Update baseline files
        run: |
          mkdir -p ${{ env.BASELINE_DIR }}

          for result_file in golden-results/results-*.json; do
            filename=$(basename "$result_file")
            version=$(echo "$filename" | sed 's/results-\([^-]*\)-.*/\1/')
            cp "$result_file" "${{ env.BASELINE_DIR }}/baseline-${version}.json"
            echo "Updated baseline for Python $version"
          done

      - name: Commit updated baselines
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore: update golden test baselines"
          file_pattern: "${{ env.BASELINE_DIR }}/*.json"

  # =============================================================================
  # Generate Report
  # =============================================================================
  report:
    name: Generate Golden Test Report
    runs-on: ubuntu-latest
    needs: [golden-tests, cross-platform-check, precision-check]
    if: always()
    steps:
      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          pattern: golden-results-*
          merge-multiple: true
          path: all-results
        continue-on-error: true

      - name: Generate summary report
        run: |
          echo "# Golden Tests Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Date:** $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> $GITHUB_STEP_SUMMARY
          echo "**Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "## Job Results" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Golden Tests | ${{ needs.golden-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Cross-Platform | ${{ needs.cross-platform-check.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Precision Check | ${{ needs.precision-check.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "## Platform Coverage" >> $GITHUB_STEP_SUMMARY
          echo "- Ubuntu (3.10, 3.11, 3.12)" >> $GITHUB_STEP_SUMMARY
          echo "- Windows (3.10, 3.11, 3.12)" >> $GITHUB_STEP_SUMMARY
          echo "- macOS (3.10, 3.11, 3.12)" >> $GITHUB_STEP_SUMMARY

      - name: Send notification on failure
        if: failure()
        uses: slackapi/slack-github-action@v1.25.0
        with:
          payload: |
            {
              "text": "Golden Tests Failed",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*GreenLang Golden Tests FAILED*\nCheck: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                  }
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK
        continue-on-error: true
