# ==============================================================================
# GL-013 PREDICTMAINT - Horizontal Pod Autoscaler
# ==============================================================================
# Automatic scaling configuration based on resource utilization
# Features:
#   - CPU-based scaling (primary metric)
#   - Memory-based scaling (secondary metric)
#   - Custom metrics support (Prometheus)
#   - Scaling behavior configuration (scale up/down rates)
# ==============================================================================
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: gl-013-predictmaint-hpa
  namespace: greenlang
  labels:
    app: gl-013-predictmaint
    app.kubernetes.io/name: gl-013-predictmaint
    app.kubernetes.io/component: autoscaler
    app.kubernetes.io/part-of: greenlang-platform
    team: iota
  annotations:
    description: "GL-013 PREDICTMAINT - Horizontal Pod Autoscaler"
spec:
  # Target deployment to scale
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: gl-013-predictmaint

  # Replica bounds
  minReplicas: 2
  maxReplicas: 10

  # ==============================================================================
  # Scaling Metrics
  # ==============================================================================
  metrics:
    # Primary metric: CPU utilization
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70

    # Secondary metric: Memory utilization
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80

    # Custom metric: Requests per second (requires Prometheus Adapter)
    - type: Pods
      pods:
        metric:
          name: http_requests_per_second
        target:
          type: AverageValue
          averageValue: "100"

    # Custom metric: Queue depth (for async processing)
    - type: External
      external:
        metric:
          name: redis_queue_length
          selector:
            matchLabels:
              queue: "gl013-predictions"
        target:
          type: AverageValue
          averageValue: "50"

  # ==============================================================================
  # Scaling Behavior - Fine-grained control over scaling
  # ==============================================================================
  behavior:
    # Scale up behavior
    scaleUp:
      # Stabilization window: Wait this long before scaling up again
      stabilizationWindowSeconds: 60
      # Policies for scaling up
      policies:
        # Add 2 pods at a time
        - type: Pods
          value: 2
          periodSeconds: 60
        # Or scale up by 50% of current replicas
        - type: Percent
          value: 50
          periodSeconds: 60
      # Use the policy that results in the most pods
      selectPolicy: Max

    # Scale down behavior
    scaleDown:
      # Stabilization window: Wait this long before scaling down
      # Longer window to prevent thrashing
      stabilizationWindowSeconds: 300
      # Policies for scaling down
      policies:
        # Remove 1 pod at a time
        - type: Pods
          value: 1
          periodSeconds: 120
        # Or scale down by 20% of current replicas
        - type: Percent
          value: 20
          periodSeconds: 120
      # Use the policy that results in the fewest pods removed
      selectPolicy: Min

---
# ==============================================================================
# Vertical Pod Autoscaler (VPA) - Optional
# ==============================================================================
# Automatically adjusts resource requests based on actual usage
# Note: VPA and HPA should not target the same resource metrics
# Use VPA for memory, HPA for CPU if using both
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: gl-013-predictmaint-vpa
  namespace: greenlang
  labels:
    app: gl-013-predictmaint
    app.kubernetes.io/name: gl-013-predictmaint
    app.kubernetes.io/component: vertical-autoscaler
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: gl-013-predictmaint
  updatePolicy:
    # Modes: Off, Initial, Recreate, Auto
    # Use "Off" initially to just get recommendations
    updateMode: "Off"
  resourcePolicy:
    containerPolicies:
      - containerName: gl-013-predictmaint
        # Minimum resources
        minAllowed:
          cpu: "250m"
          memory: "256Mi"
        # Maximum resources
        maxAllowed:
          cpu: "2000m"
          memory: "2Gi"
        # Which resources VPA can modify
        controlledResources:
          - cpu
          - memory
        # Mode for each container: Auto, Off
        mode: "Auto"
