# ==============================================================================
# GL-014 EXCHANGER-PRO - Horizontal Pod Autoscaler
# ==============================================================================
# Automatic scaling configuration based on resource utilization
# Features:
#   - CPU-based scaling (target: 70%)
#   - Memory-based scaling (target: 80%)
#   - Custom metrics support (Prometheus)
#   - Scaling behavior configuration (scale up/down rates)
#   - Scale down stabilization (300 seconds)
# Replica Configuration:
#   - Minimum replicas: 2
#   - Maximum replicas: 10
# ==============================================================================
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: gl-014-exchanger-pro-hpa
  namespace: greenlang
  labels:
    app: gl-014-exchanger-pro
    app.kubernetes.io/name: gl-014-exchanger-pro
    app.kubernetes.io/component: autoscaler
    app.kubernetes.io/part-of: greenlang-platform
    team: iota
  annotations:
    description: "GL-014 EXCHANGER-PRO - Horizontal Pod Autoscaler"
spec:
  # Target deployment to scale
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: gl-014-exchanger-pro

  # Replica bounds
  minReplicas: 2
  maxReplicas: 10

  # ==============================================================================
  # Scaling Metrics
  # ==============================================================================
  metrics:
    # Primary metric: CPU utilization (target: 70%)
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70

    # Secondary metric: Memory utilization (target: 80%)
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80

    # Custom metric: Requests per second (requires Prometheus Adapter)
    - type: Pods
      pods:
        metric:
          name: http_requests_per_second
        target:
          type: AverageValue
          averageValue: "100"

    # Custom metric: Heat exchanger analysis queue depth
    - type: External
      external:
        metric:
          name: redis_queue_length
          selector:
            matchLabels:
              queue: "gl014-analysis-queue"
        target:
          type: AverageValue
          averageValue: "50"

    # Custom metric: Active heat exchanger calculations
    - type: Pods
      pods:
        metric:
          name: active_calculations
        target:
          type: AverageValue
          averageValue: "10"

  # ==============================================================================
  # Scaling Behavior - Fine-grained control over scaling
  # ==============================================================================
  behavior:
    # Scale up behavior
    scaleUp:
      # Stabilization window: Wait this long before scaling up again
      stabilizationWindowSeconds: 60
      # Policies for scaling up
      policies:
        # Add 2 pods at a time
        - type: Pods
          value: 2
          periodSeconds: 60
        # Or scale up by 50% of current replicas
        - type: Percent
          value: 50
          periodSeconds: 60
      # Use the policy that results in the most pods
      selectPolicy: Max

    # Scale down behavior
    scaleDown:
      # Stabilization window: Wait this long before scaling down
      # Longer window (300s) to prevent thrashing
      stabilizationWindowSeconds: 300
      # Policies for scaling down
      policies:
        # Remove 1 pod at a time
        - type: Pods
          value: 1
          periodSeconds: 120
        # Or scale down by 20% of current replicas
        - type: Percent
          value: 20
          periodSeconds: 120
      # Use the policy that results in the fewest pods removed
      selectPolicy: Min

---
# ==============================================================================
# Vertical Pod Autoscaler (VPA) - Optional
# ==============================================================================
# Automatically adjusts resource requests based on actual usage
# Note: VPA and HPA should not target the same resource metrics
# Use VPA for memory, HPA for CPU if using both
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: gl-014-exchanger-pro-vpa
  namespace: greenlang
  labels:
    app: gl-014-exchanger-pro
    app.kubernetes.io/name: gl-014-exchanger-pro
    app.kubernetes.io/component: vertical-autoscaler
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: gl-014-exchanger-pro
  updatePolicy:
    # Modes: Off, Initial, Recreate, Auto
    # Use "Off" initially to just get recommendations
    updateMode: "Off"
  resourcePolicy:
    containerPolicies:
      - containerName: gl-014-exchanger-pro
        # Minimum resources
        minAllowed:
          cpu: "500m"
          memory: "1Gi"
        # Maximum resources
        maxAllowed:
          cpu: "4000m"
          memory: "4Gi"
        # Which resources VPA can modify
        controlledResources:
          - cpu
          - memory
        # Mode for each container: Auto, Off
        mode: "Auto"

---
# ==============================================================================
# Prometheus Adapter Custom Metrics Configuration
# ==============================================================================
# This ConfigMap defines custom metrics for the Prometheus Adapter
# Deploy this if you need custom metrics-based scaling
apiVersion: v1
kind: ConfigMap
metadata:
  name: gl-014-exchanger-pro-adapter-config
  namespace: monitoring
  labels:
    app: gl-014-exchanger-pro
    app.kubernetes.io/component: prometheus-adapter
data:
  config.yaml: |
    rules:
    # HTTP Requests per Second
    - seriesQuery: 'http_requests_total{namespace="greenlang",pod=~"gl-014-exchanger-pro.*"}'
      resources:
        overrides:
          namespace:
            resource: namespace
          pod:
            resource: pod
      name:
        matches: "^(.*)_total$"
        as: "${1}_per_second"
      metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>}[2m])) by (<<.GroupBy>>)'

    # Active Calculations
    - seriesQuery: 'gl014_active_calculations{namespace="greenlang",pod=~"gl-014-exchanger-pro.*"}'
      resources:
        overrides:
          namespace:
            resource: namespace
          pod:
            resource: pod
      name:
        matches: "^(.*)$"
        as: "active_calculations"
      metricsQuery: 'avg(<<.Series>>{<<.LabelMatchers>>}) by (<<.GroupBy>>)'

    # Heat Exchanger Analysis Queue Depth
    - seriesQuery: 'gl014_analysis_queue_length{namespace="greenlang"}'
      resources:
        overrides:
          namespace:
            resource: namespace
      name:
        matches: "^(.*)$"
        as: "redis_queue_length"
      metricsQuery: 'avg(<<.Series>>{<<.LabelMatchers>>,queue="gl014-analysis-queue"})'

    # Fouling Calculation Latency (p95)
    - seriesQuery: 'gl014_fouling_calculation_duration_seconds_bucket{namespace="greenlang",pod=~"gl-014-exchanger-pro.*"}'
      resources:
        overrides:
          namespace:
            resource: namespace
          pod:
            resource: pod
      name:
        matches: "^(.*)_bucket$"
        as: "fouling_calculation_latency_p95"
      metricsQuery: 'histogram_quantile(0.95, sum(rate(<<.Series>>{<<.LabelMatchers>>}[5m])) by (le, <<.GroupBy>>))'
