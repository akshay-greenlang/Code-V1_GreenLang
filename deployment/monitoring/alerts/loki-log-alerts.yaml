# =============================================================================
# GreenLang Climate OS - Loki Log-Based Prometheus Alert Rules
# =============================================================================
# PRD: INFRA-009 Centralized Logging & Log-Based Alerting
# Defines alerting rules sourced from Loki log streams via Loki Ruler.
# These rules evaluate LogQL expressions and fire Prometheus-compatible alerts.
# =============================================================================

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: loki-log-alerts
  namespace: monitoring
  labels:
    app.kubernetes.io/name: loki
    app.kubernetes.io/part-of: greenlang
    prometheus: kube-prometheus
    role: alert-rules
    greenlang.io/component: infra-009
spec:
  groups:
    # =========================================================================
    # Group 1: Log Error Alerts
    # Detects elevated error rates, warning floods, and critical log events
    # across all GreenLang services in the greenlang namespace.
    # =========================================================================
    - name: greenlang-log-errors
      interval: 30s
      rules:
        # -------------------------------------------------------------------
        # HighErrorRate
        # Fires when any single service emits more than 0.5 error-level log
        # lines per second sustained over 5 minutes. Indicates a service is
        # experiencing persistent failures that need immediate attention.
        # -------------------------------------------------------------------
        - alert: HighErrorRate
          expr: |
            sum by (app) (
              rate(
                {namespace="greenlang"} | json | level="error" [5m]
              )
            ) > 0.5
          for: 5m
          labels:
            severity: critical
            team: platform
            component: logging
            prd: infra-009
          annotations:
            summary: "High error log rate for service {{ $labels.app }}"
            description: >-
              Service {{ $labels.app }} is emitting error logs at a rate of
              {{ $value | printf "%.2f" }} lines/s over the last 5 minutes,
              exceeding the 0.5/s threshold. This indicates persistent failures
              that require immediate investigation.
            impact: "Service stability degraded. Potential data loss or user-facing errors."
            action: "Check application logs for root cause. Review recent deployments and config changes."
            runbook_url: "https://runbooks.greenlang.io/logs/high-error-rate"
            dashboard_url: "https://grafana.greenlang.io/d/greenlang-log-exploration"

        # -------------------------------------------------------------------
        # HighWarningRate
        # Fires when warning-level logs exceed 5/s for 10 minutes. Warnings
        # at this rate often precede cascading failures and should be triaged.
        # -------------------------------------------------------------------
        - alert: HighWarningRate
          expr: |
            sum by (app) (
              rate(
                {namespace="greenlang"} | json | level="warning" [5m]
              )
            ) > 5
          for: 10m
          labels:
            severity: warning
            team: platform
            component: logging
            prd: infra-009
          annotations:
            summary: "High warning log rate for service {{ $labels.app }}"
            description: >-
              Service {{ $labels.app }} is emitting warning logs at a rate of
              {{ $value | printf "%.2f" }} lines/s over the last 10 minutes,
              exceeding the 5/s threshold. Sustained warning floods often
              precede cascading failures.
            impact: "Potential service degradation. May escalate to errors."
            action: "Investigate warning messages. Check downstream dependencies and resource utilization."
            runbook_url: "https://runbooks.greenlang.io/logs/high-warning-rate"
            dashboard_url: "https://grafana.greenlang.io/d/greenlang-log-exploration"

        # -------------------------------------------------------------------
        # CriticalLogDetected
        # Fires immediately when any CRITICAL-level log is emitted in the
        # greenlang namespace. CRITICAL logs indicate unrecoverable errors
        # such as data corruption, security breaches, or total service loss.
        # -------------------------------------------------------------------
        - alert: CriticalLogDetected
          expr: |
            count_over_time(
              {namespace="greenlang"} | json | level="critical" [1m]
            ) > 0
          for: 0m
          labels:
            severity: critical
            team: platform
            component: logging
            prd: infra-009
          annotations:
            summary: "CRITICAL log detected in {{ $labels.app }}"
            description: >-
              A CRITICAL-level log entry was detected in service {{ $labels.app }}
              within the greenlang namespace. CRITICAL logs indicate unrecoverable
              errors that may result in data corruption, security breaches, or
              complete service loss. Immediate human intervention is required.
            impact: "Potential data loss, security breach, or total service failure."
            action: "Page on-call engineer immediately. Investigate root cause and consider rollback."
            runbook_url: "https://runbooks.greenlang.io/logs/critical-log-detected"
            dashboard_url: "https://grafana.greenlang.io/d/greenlang-log-exploration"

        # -------------------------------------------------------------------
        # ErrorBurstDetected
        # Fires when more than 50 error log lines are emitted by a single
        # service within a 5-minute window. Catches sudden bursts that might
        # not sustain a high rate but indicate transient failures.
        # -------------------------------------------------------------------
        - alert: ErrorBurstDetected
          expr: |
            sum by (app) (
              count_over_time(
                {namespace="greenlang"} | json | level="error" [5m]
              )
            ) > 50
          for: 0m
          labels:
            severity: warning
            team: platform
            component: logging
            prd: infra-009
          annotations:
            summary: "Error burst detected in service {{ $labels.app }}"
            description: >-
              Service {{ $labels.app }} has emitted {{ $value }} error log lines
              in the last 5 minutes, exceeding the 50-line threshold. This burst
              may indicate a transient failure, retry storm, or dependency outage.
            impact: "Service may be experiencing intermittent failures."
            action: "Check for transient errors, retry storms, or downstream dependency issues."
            runbook_url: "https://runbooks.greenlang.io/logs/error-burst"
            dashboard_url: "https://grafana.greenlang.io/d/greenlang-log-exploration"

    # =========================================================================
    # Group 2: Compliance Alerts
    # Monitors log-based signals from GreenLang compliance agents (CSRD, CBAM,
    # EUDR, SB253). These services have strict regulatory SLAs and failures
    # must be escalated immediately.
    # =========================================================================
    - name: greenlang-compliance-alerts
      interval: 30s
      rules:
        # -------------------------------------------------------------------
        # CSRDProcessingFailure
        # CSRD (Corporate Sustainability Reporting Directive) processing
        # failures can lead to regulatory non-compliance and fines. More than
        # 3 failures in 15 minutes triggers a critical alert.
        # -------------------------------------------------------------------
        - alert: CSRDProcessingFailure
          expr: |
            count_over_time(
              {namespace="greenlang", app="csrd-agent"} |= "processing_failed" | json | level="error" [15m]
            ) > 3
          for: 0m
          labels:
            severity: critical
            team: compliance
            component: csrd-agent
            compliance: csrd
            prd: infra-009
          annotations:
            summary: "CSRD processing failures exceed threshold"
            description: >-
              The csrd-agent has logged {{ $value }} processing_failed errors
              in the last 15 minutes, exceeding the threshold of 3. CSRD report
              generation may be impaired, risking regulatory non-compliance.
            impact: "CSRD regulatory reports may be delayed or incorrect. Potential compliance violation."
            action: "Check CSRD agent logs. Verify data source connectivity and report template integrity."
            runbook_url: "https://runbooks.greenlang.io/compliance/csrd-processing-failure"
            dashboard_url: "https://grafana.greenlang.io/d/greenlang-log-exploration?var-app=csrd-agent"

        # -------------------------------------------------------------------
        # CBAMCalculationError
        # CBAM (Carbon Border Adjustment Mechanism) calculation errors affect
        # carbon pricing and import duty computations.
        # -------------------------------------------------------------------
        - alert: CBAMCalculationError
          expr: |
            count_over_time(
              {namespace="greenlang", app="cbam-agent"} |= "calculation_error" | json | level="error" [15m]
            ) > 3
          for: 0m
          labels:
            severity: critical
            team: compliance
            component: cbam-agent
            compliance: cbam
            prd: infra-009
          annotations:
            summary: "CBAM calculation errors exceed threshold"
            description: >-
              The cbam-agent has logged {{ $value }} calculation_error events
              in the last 15 minutes, exceeding the threshold of 3. Carbon border
              adjustment calculations may be producing incorrect results.
            impact: "CBAM duty calculations may be incorrect. Financial and compliance risk."
            action: "Review CBAM agent logs. Validate emission factor data and calculation pipeline."
            runbook_url: "https://runbooks.greenlang.io/compliance/cbam-calculation-error"
            dashboard_url: "https://grafana.greenlang.io/d/greenlang-log-exploration?var-app=cbam-agent"

        # -------------------------------------------------------------------
        # EUDRComplianceFailure
        # EUDR (EU Deforestation Regulation) compliance check failures are
        # high-severity since even a single missed check can block supply
        # chain operations.
        # -------------------------------------------------------------------
        - alert: EUDRComplianceFailure
          expr: |
            count_over_time(
              {namespace="greenlang", app="eudr-agent"} |= "compliance_check_failed" | json | level="error" [10m]
            ) > 1
          for: 0m
          labels:
            severity: critical
            team: compliance
            component: eudr-agent
            compliance: eudr
            prd: infra-009
          annotations:
            summary: "EUDR compliance check failures detected"
            description: >-
              The eudr-agent has logged {{ $value }} compliance_check_failed
              events in the last 10 minutes. EUDR deforestation-free compliance
              verification is failing, which may block supply chain operations.
            impact: "Supply chain operations may be blocked. EUDR regulatory violation risk."
            action: "Investigate EUDR agent logs. Check geospatial data sources and verification pipeline."
            runbook_url: "https://runbooks.greenlang.io/compliance/eudr-compliance-failure"
            dashboard_url: "https://grafana.greenlang.io/d/greenlang-log-exploration?var-app=eudr-agent"

        # -------------------------------------------------------------------
        # SB253DisclosureError
        # SB253 (California Climate Corporate Data Accountability Act) requires
        # accurate Scope 1/2/3 emissions disclosure. Errors in disclosure
        # generation are critical.
        # -------------------------------------------------------------------
        - alert: SB253DisclosureError
          expr: |
            count_over_time(
              {namespace="greenlang", app="sb253-agent"} |= "disclosure_error" | json | level="error" [15m]
            ) > 3
          for: 0m
          labels:
            severity: critical
            team: compliance
            component: sb253-agent
            compliance: sb253
            prd: infra-009
          annotations:
            summary: "SB253 disclosure errors exceed threshold"
            description: >-
              The sb253-agent has logged {{ $value }} disclosure_error events
              in the last 15 minutes, exceeding the threshold of 3. Emissions
              disclosure generation for SB253 compliance is failing.
            impact: "SB253 emissions disclosures may be delayed or inaccurate. Regulatory risk."
            action: "Check SB253 agent logs. Verify emissions data pipeline and disclosure templates."
            runbook_url: "https://runbooks.greenlang.io/compliance/sb253-disclosure-error"
            dashboard_url: "https://grafana.greenlang.io/d/greenlang-log-exploration?var-app=sb253-agent"

        # -------------------------------------------------------------------
        # ComplianceAuditGap
        # Fires when no audit log entries have been received from any
        # compliance service for 15 minutes. Audit trail continuity is a
        # regulatory requirement.
        # -------------------------------------------------------------------
        - alert: ComplianceAuditGap
          expr: |
            (
              count_over_time(
                {namespace="greenlang", app=~"csrd-agent|cbam-agent|eudr-agent|sb253-agent"} |= "audit" | json [15m]
              )
            ) == 0
          for: 15m
          labels:
            severity: warning
            team: compliance
            component: audit
            prd: infra-009
          annotations:
            summary: "No compliance audit logs received for 15 minutes"
            description: >-
              No audit log entries have been received from compliance services
              (csrd-agent, cbam-agent, eudr-agent, sb253-agent) for the last
              15 minutes. Audit trail continuity is a regulatory requirement
              and gaps must be investigated.
            impact: "Audit trail gap. May violate regulatory record-keeping requirements."
            action: "Verify compliance services are running. Check log pipeline ingestion. Review agent health endpoints."
            runbook_url: "https://runbooks.greenlang.io/compliance/audit-gap"
            dashboard_url: "https://grafana.greenlang.io/d/greenlang-log-exploration?var-app=csrd-agent|cbam-agent|eudr-agent|sb253-agent"

    # =========================================================================
    # Group 3: Security Alerts
    # Detects authentication failures, authorization denials, potential data
    # leaks, and suspicious activity patterns from log streams.
    # =========================================================================
    - name: greenlang-security-alerts
      interval: 15s
      rules:
        # -------------------------------------------------------------------
        # AuthenticationFailureBurst
        # More than 10 authentication failures in 5 minutes may indicate a
        # brute-force attack or credential stuffing attempt.
        # -------------------------------------------------------------------
        - alert: AuthenticationFailureBurst
          expr: |
            sum by (app) (
              count_over_time(
                {namespace="greenlang"} |~ "authentication_failed|auth_failure" | json [5m]
              )
            ) > 10
          for: 0m
          labels:
            severity: critical
            team: security
            component: authentication
            prd: infra-009
          annotations:
            summary: "Authentication failure burst detected in {{ $labels.app }}"
            description: >-
              Service {{ $labels.app }} has logged {{ $value }} authentication
              failure events in the last 5 minutes, exceeding the threshold of 10.
              This may indicate a brute-force attack, credential stuffing, or
              misconfigured client credentials.
            impact: "Potential security breach attempt. Legitimate users may be locked out."
            action: "Check source IPs. Review WAF logs. Consider temporary IP blocks. Verify credential rotation."
            runbook_url: "https://runbooks.greenlang.io/security/auth-failure-burst"
            dashboard_url: "https://grafana.greenlang.io/d/greenlang-log-exploration?var-level=error"

        # -------------------------------------------------------------------
        # AuthorizationDenied
        # Elevated authorization denial rates may indicate privilege escalation
        # attempts or misconfigured RBAC policies.
        # -------------------------------------------------------------------
        - alert: AuthorizationDenied
          expr: |
            sum by (app) (
              count_over_time(
                {namespace="greenlang"} |~ "authorization_denied|access_denied" | json [10m]
              )
            ) > 20
          for: 0m
          labels:
            severity: warning
            team: security
            component: authorization
            prd: infra-009
          annotations:
            summary: "Elevated authorization denials in {{ $labels.app }}"
            description: >-
              Service {{ $labels.app }} has logged {{ $value }} authorization
              denied events in the last 10 minutes, exceeding the threshold of 20.
              This may indicate privilege escalation attempts or RBAC misconfigurations.
            impact: "Potential unauthorized access attempts. RBAC policies may need review."
            action: "Audit denied requests for patterns. Check RBAC role bindings. Review tenant isolation."
            runbook_url: "https://runbooks.greenlang.io/security/authorization-denied"
            dashboard_url: "https://grafana.greenlang.io/d/greenlang-log-exploration?var-level=warning"

        # -------------------------------------------------------------------
        # SensitiveDataLeak
        # Detects potential PII or sensitive data appearing in log output
        # that was not properly redacted. Matches email addresses, credit
        # card numbers, and API key patterns.
        # -------------------------------------------------------------------
        - alert: SensitiveDataLeak
          expr: |
            count_over_time(
              {namespace="greenlang"}
                |~ "([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z]{2,})"
                |~ "(\\b[0-9]{4}[- ]?[0-9]{4}[- ]?[0-9]{4}[- ]?[0-9]{4}\\b)"
                |~ "(sk_live_|pk_live_|AKIA[A-Z0-9]{16}|ghp_[a-zA-Z0-9]{36})"
              [5m]
            ) > 0
          for: 0m
          labels:
            severity: critical
            team: security
            component: data-protection
            prd: infra-009
          annotations:
            summary: "Potential sensitive data leak detected in logs"
            description: >-
              Log entries in the greenlang namespace contain patterns matching
              email addresses, credit card numbers, or API keys that were not
              properly redacted. This is a data protection violation requiring
              immediate remediation.
            impact: "PII or credentials may be exposed in log storage. Potential GDPR/data protection violation."
            action: "Identify the source service. Patch log redaction filters. Purge affected log entries from storage."
            runbook_url: "https://runbooks.greenlang.io/security/sensitive-data-leak"
            dashboard_url: "https://grafana.greenlang.io/d/greenlang-log-exploration"

        # -------------------------------------------------------------------
        # SuspiciousActivity
        # Detects when a single tenant generates an abnormally high rate of
        # 4xx/5xx status codes, which may indicate abuse, fuzzing, or
        # compromised credentials.
        # -------------------------------------------------------------------
        - alert: SuspiciousActivity
          expr: |
            sum by (tenant_id) (
              rate(
                {namespace="greenlang"} | json | status_code >= 400 [5m]
              )
            ) > 10
          for: 5m
          labels:
            severity: warning
            team: security
            component: abuse-detection
            prd: infra-009
          annotations:
            summary: "Suspicious activity from tenant {{ $labels.tenant_id }}"
            description: >-
              Tenant {{ $labels.tenant_id }} is generating error status codes
              (4xx/5xx) at a rate of {{ $value | printf "%.2f" }}/s over the
              last 5 minutes, exceeding the 10/s threshold. This may indicate
              abuse, API fuzzing, or compromised tenant credentials.
            impact: "Potential abuse or security incident. May affect platform stability for other tenants."
            action: "Review tenant activity logs. Consider rate limiting or temporary suspension. Contact tenant if legitimate."
            runbook_url: "https://runbooks.greenlang.io/security/suspicious-activity"
            dashboard_url: "https://grafana.greenlang.io/d/greenlang-log-exploration"

    # =========================================================================
    # Group 4: Infrastructure Alerts (Loki Health)
    # Monitors the health and performance of the Loki logging infrastructure
    # itself. Uses Prometheus metrics exported by Loki components.
    # =========================================================================
    - name: greenlang-infrastructure-alerts
      interval: 30s
      rules:
        # -------------------------------------------------------------------
        # LogIngestionFailure
        # Fires when Loki stops receiving log data entirely. A total ingestion
        # stop means logs are being lost and all log-based alerts are blind.
        # -------------------------------------------------------------------
        - alert: LogIngestionFailure
          expr: |
            rate(loki_distributor_bytes_received_total{namespace="monitoring"}[5m]) == 0
          for: 5m
          labels:
            severity: critical
            team: platform
            component: loki
            prd: infra-009
          annotations:
            summary: "Loki log ingestion has stopped"
            description: >-
              Loki distributor has received zero bytes in the last 5 minutes.
              Log ingestion has completely stopped, which means all log-based
              alerting is effectively blind and logs are being lost.
            impact: "Complete loss of log visibility. All log-based alerts are non-functional."
            action: "Check Loki distributor pods. Verify Promtail/Alloy agents. Check network policies and ingress."
            runbook_url: "https://runbooks.greenlang.io/loki/ingestion-failure"
            dashboard_url: "https://grafana.greenlang.io/d/loki-operations"

        # -------------------------------------------------------------------
        # LogIngestionRateLimited
        # Fires when Loki discards samples due to rate limiting or tenant
        # quotas. Discarded logs mean gaps in log coverage.
        # -------------------------------------------------------------------
        - alert: LogIngestionRateLimited
          expr: |
            sum by (tenant) (
              rate(loki_discarded_samples_total{namespace="monitoring"}[5m])
            ) > 0
          for: 5m
          labels:
            severity: warning
            team: platform
            component: loki
            prd: infra-009
          annotations:
            summary: "Loki is discarding log samples for tenant {{ $labels.tenant }}"
            description: >-
              Loki is discarding log samples for tenant {{ $labels.tenant }} at
              a rate of {{ $value | printf "%.2f" }}/s. This indicates the tenant
              is exceeding ingestion rate limits or stream limits.
            impact: "Log data is being lost for this tenant. Log-based alerts may miss events."
            action: "Review tenant ingestion limits. Check for log volume spikes. Consider increasing limits."
            runbook_url: "https://runbooks.greenlang.io/loki/rate-limited"
            dashboard_url: "https://grafana.greenlang.io/d/loki-operations"

        # -------------------------------------------------------------------
        # LokiQuerySlowdown
        # Fires when Loki query p99 latency exceeds 10 seconds, indicating
        # degraded query performance.
        # -------------------------------------------------------------------
        - alert: LokiQuerySlowdown
          expr: |
            histogram_quantile(0.99,
              sum by (le, route) (
                rate(loki_request_duration_seconds_bucket{namespace="monitoring", method="GET"}[10m])
              )
            ) > 10
          for: 10m
          labels:
            severity: warning
            team: platform
            component: loki
            prd: infra-009
          annotations:
            summary: "Loki query latency p99 exceeds 10s on route {{ $labels.route }}"
            description: >-
              Loki query latency p99 for route {{ $labels.route }} is
              {{ $value | printf "%.1f" }}s, exceeding the 10-second threshold
              for the last 10 minutes. Dashboard and alert queries may be
              timing out.
            impact: "Grafana dashboards and alert evaluations may be slow or failing."
            action: "Check Loki querier resource usage. Review query patterns. Consider adding query frontend caching."
            runbook_url: "https://runbooks.greenlang.io/loki/query-slowdown"
            dashboard_url: "https://grafana.greenlang.io/d/loki-operations"

        # -------------------------------------------------------------------
        # LokiCompactorBehind
        # Fires when the compactor has running compactions stuck for 30
        # minutes, indicating a compaction backlog.
        # -------------------------------------------------------------------
        - alert: LokiCompactorBehind
          expr: |
            loki_compactor_running_compactions{namespace="monitoring"} > 0
          for: 30m
          labels:
            severity: warning
            team: platform
            component: loki
            prd: infra-009
          annotations:
            summary: "Loki compactor has compactions stuck for 30 minutes"
            description: >-
              The Loki compactor has had running compactions for over 30 minutes
              continuously. This may indicate a compaction backlog, insufficient
              resources, or object storage issues.
            impact: "Increased storage usage. Slower queries over older data. Potential retention policy delays."
            action: "Check compactor pod resources. Review object storage latency. Consider scaling compactor."
            runbook_url: "https://runbooks.greenlang.io/loki/compactor-behind"
            dashboard_url: "https://grafana.greenlang.io/d/loki-operations"

        # -------------------------------------------------------------------
        # DiskUsageHigh
        # Fires when Loki PVC usage exceeds 85%, risking ingestion failures
        # if disk fills up completely.
        # -------------------------------------------------------------------
        - alert: DiskUsageHigh
          expr: |
            (
              kubelet_volume_stats_used_bytes{namespace="monitoring", persistentvolumeclaim=~".*loki.*"}
              /
              kubelet_volume_stats_capacity_bytes{namespace="monitoring", persistentvolumeclaim=~".*loki.*"}
            ) * 100 > 85
          for: 10m
          labels:
            severity: warning
            team: platform
            component: loki
            prd: infra-009
          annotations:
            summary: "Loki PVC {{ $labels.persistentvolumeclaim }} usage above 85%"
            description: >-
              Loki persistent volume {{ $labels.persistentvolumeclaim }} is at
              {{ $value | printf "%.1f" }}% capacity. If disk fills completely,
              Loki will stop ingesting logs and log-based alerts will fail.
            impact: "Imminent risk of log ingestion failure if disk fills up."
            action: "Expand PVC size. Review retention policies. Check for ingestion volume spikes."
            runbook_url: "https://runbooks.greenlang.io/loki/disk-usage-high"
            dashboard_url: "https://grafana.greenlang.io/d/loki-operations"
