# =============================================================================
# TimescaleDB Performance Tuning Configuration
# =============================================================================
# GreenLang TimescaleDB Extension Configuration
# Optimized for: Time-Series Data, IoT Workloads, Analytics
# Last Updated: 2026-02-03
# =============================================================================

# -----------------------------------------------------------------------------
# SHARED PRELOAD LIBRARIES
# -----------------------------------------------------------------------------
# Load TimescaleDB and additional extensions at startup
# Note: pg_stat_statements provides query performance insights
shared_preload_libraries = 'timescaledb,pg_stat_statements'

# -----------------------------------------------------------------------------
# TIMESCALEDB BACKGROUND WORKERS
# -----------------------------------------------------------------------------
# Maximum background workers for TimescaleDB operations
# Includes: compression, continuous aggregates, data retention
# Recommendation: Set to number of hypertables that need background work
timescaledb.max_background_workers = 8

# License level (apache = open source features only)
# Options: apache, timescale (includes licensed features)
timescaledb.license = 'timescale'

# -----------------------------------------------------------------------------
# TELEMETRY SETTINGS
# -----------------------------------------------------------------------------
# Disable telemetry for production/privacy
# Options: off, basic, full
timescaledb.telemetry_level = off

# -----------------------------------------------------------------------------
# COMPRESSION SETTINGS
# -----------------------------------------------------------------------------
# Default compression settings for hypertables
# These can be overridden per-hypertable

# Default segment-by columns (empty = auto-detect)
# Typically set to dimensions that filter most queries
timescaledb.compress_segmentby_default = ''

# Default order-by columns for compression (empty = auto-detect)
# Typically set to time column for time-series data
timescaledb.compress_orderby_default = ''

# Enable compression on chunks older than specified interval
# Set per-hypertable using add_compression_policy()

# -----------------------------------------------------------------------------
# CHUNK MANAGEMENT
# -----------------------------------------------------------------------------
# Default chunk time interval (can be overridden per-hypertable)
# Recommendation: 1 day to 1 week depending on data volume
# Set using create_hypertable() or set_chunk_time_interval()

# Maximum number of chunks to insert into in parallel
# Higher values = better insert throughput, more memory usage
timescaledb.max_insert_batch_size = 1000

# Enable chunk preloading for improved query performance
# timescaledb.enable_chunk_preload = on

# -----------------------------------------------------------------------------
# CONTINUOUS AGGREGATES
# -----------------------------------------------------------------------------
# Enable materialized views that automatically update
# Configure using CREATE MATERIALIZED VIEW ... WITH timescaledb.continuous

# Background worker interval for refreshing continuous aggregates
# Refresh policies are set per-aggregate using add_continuous_aggregate_policy()

# Maximum number of continuous aggregate jobs running concurrently
# timescaledb.max_cagg_jobs = 4

# -----------------------------------------------------------------------------
# DATA RETENTION
# -----------------------------------------------------------------------------
# Automatic data retention through drop_chunks_policy
# Configure per-hypertable using add_retention_policy()

# Enable scheduled jobs for retention
# timescaledb.enable_scheduled_jobs = on

# -----------------------------------------------------------------------------
# DISTRIBUTED HYPERTABLES (Multi-Node)
# -----------------------------------------------------------------------------
# For distributed TimescaleDB deployments

# Enable distributed DDL operations
# timescaledb.enable_2pc = on

# Passthrough for distributed queries
# timescaledb.enable_remote_explain = on

# Maximum connections to data nodes
# timescaledb.max_data_node_connections = 16

# Connection timeout for data nodes
# timescaledb.data_node_connection_timeout = 60s

# -----------------------------------------------------------------------------
# QUERY OPTIMIZATION
# -----------------------------------------------------------------------------
# Enable constraint exclusion for chunk pruning
constraint_exclusion = partition

# Enable TimescaleDB-specific query optimizations
timescaledb.enable_optimizations = on

# Enable chunk append optimization
timescaledb.enable_chunk_append = on

# Enable parallel chunk append
timescaledb.enable_parallel_chunk_append = on

# Enable constraint-aware append
timescaledb.enable_constraint_aware_append = on

# Enable ordered append for sorted queries
timescaledb.enable_ordered_append = on

# Enable runtime chunk exclusion
timescaledb.enable_runtime_exclusion = on

# Enable optimization for DISTINCT queries
timescaledb.enable_qual_propagation = on

# Enable cagg watermark optimization
timescaledb.enable_cagg_watermark_constify = on

# Enable transparent decompression in queries
timescaledb.enable_transparent_decompression = on

# Enable DML decompression (for UPDATE/DELETE on compressed chunks)
timescaledb.enable_dml_decompression = on

# Enable bulk decompression for full chunk scans
timescaledb.enable_bulk_decompression = on

# -----------------------------------------------------------------------------
# MEMORY SETTINGS
# -----------------------------------------------------------------------------
# Memory context for TimescaleDB operations
# Uses work_mem from postgresql.conf as baseline

# Sort memory for compression operations
# Increase for faster compression with wide tables
# timescaledb.compress_sort_mem = 'work_mem'

# -----------------------------------------------------------------------------
# LOGGING AND DEBUGGING
# -----------------------------------------------------------------------------
# Enable detailed logging for TimescaleDB operations
# Useful for debugging but increases log volume
# timescaledb.debug_require_batch_sorted_merge = off
# timescaledb.debug_allow_cagg_with_deprecated_funcs = off

# -----------------------------------------------------------------------------
# PG_STAT_STATEMENTS CONFIGURATION
# -----------------------------------------------------------------------------
# Track query statistics for performance analysis

# Maximum number of statements tracked
pg_stat_statements.max = 10000

# Track all statements or just top-level
pg_stat_statements.track = all

# Track utility statements (COPY, etc.)
pg_stat_statements.track_utility = on

# Save statistics across restarts
pg_stat_statements.save = on

# Track planning statistics
pg_stat_statements.track_planning = on

# -----------------------------------------------------------------------------
# HYPERTABLE BEST PRACTICES
# -----------------------------------------------------------------------------
# Example: Creating an optimized hypertable
#
# -- Create hypertable with 1-day chunks
# SELECT create_hypertable('sensor_data', 'time',
#     chunk_time_interval => INTERVAL '1 day',
#     create_default_indexes => TRUE
# );
#
# -- Add compression policy (compress chunks older than 7 days)
# ALTER TABLE sensor_data SET (
#     timescaledb.compress,
#     timescaledb.compress_segmentby = 'device_id',
#     timescaledb.compress_orderby = 'time DESC'
# );
# SELECT add_compression_policy('sensor_data', INTERVAL '7 days');
#
# -- Add retention policy (drop chunks older than 90 days)
# SELECT add_retention_policy('sensor_data', INTERVAL '90 days');
#
# -- Create continuous aggregate for hourly rollups
# CREATE MATERIALIZED VIEW sensor_data_hourly
# WITH (timescaledb.continuous) AS
# SELECT
#     time_bucket('1 hour', time) AS bucket,
#     device_id,
#     AVG(value) AS avg_value,
#     MAX(value) AS max_value,
#     MIN(value) AS min_value,
#     COUNT(*) AS sample_count
# FROM sensor_data
# GROUP BY bucket, device_id;
#
# -- Add refresh policy for continuous aggregate
# SELECT add_continuous_aggregate_policy('sensor_data_hourly',
#     start_offset => INTERVAL '3 hours',
#     end_offset => INTERVAL '1 hour',
#     schedule_interval => INTERVAL '1 hour'
# );

# =============================================================================
# END OF TIMESCALEDB CONFIGURATION
# =============================================================================
