#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test Generator

Auto-generate comprehensive tests for GreenLang agents.
Analyzes agent code and creates unit tests, integration tests, and fixtures.
"""

import argparse
import ast
import os
import sys
from pathlib import Path
from typing import Dict, Any, List, Tuple


class AgentAnalyzer:
    """Analyze agent code to extract information for test generation."""

    def __init__(self, file_path: str):
        self.file_path = file_path
        self.agent_name = None
        self.methods = []
        self.has_llm = False
        self.has_cache = False
        self.has_database = False

    def analyze(self) -> Dict[str, Any]:
        """Analyze agent file."""

        with open(self.file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        tree = ast.parse(content)

        # Find agent class
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                # Check if it's an agent class (inherits from BaseAgent)
                for base in node.bases:
                    if isinstance(base, ast.Name) and 'Agent' in base.id:
                        self.agent_name = node.name
                        self._analyze_class(node)
                        break

        # Check imports for features
        self.has_llm = 'ChatSession' in content or 'llm' in content
        self.has_cache = 'CacheManager' in content or 'cache' in content
        self.has_database = 'database' in content.lower()

        return {
            'agent_name': self.agent_name,
            'methods': self.methods,
            'has_llm': self.has_llm,
            'has_cache': self.has_cache,
            'has_database': self.has_database
        }

    def _analyze_class(self, class_node):
        """Analyze class methods."""

        for node in class_node.body:
            if isinstance(node, ast.FunctionDef):
                if not node.name.startswith('_') or node.name in ['__init__']:
                    self.methods.append(node.name)


class TestGenerator:
    """Generate test files."""

    @staticmethod
    def generate_unit_tests(agent_info: Dict[str, Any], file_path: str) -> str:
        """Generate unit tests."""

        agent_name = agent_info['agent_name']
        methods = agent_info['methods']
        module_path = Path(file_path).stem

        return f'''"""
Unit Tests for {agent_name}

Auto-generated by GreenLang Test Generator
Customize as needed for your specific use cases.
"""

import pytest
import json
from unittest.mock import Mock, patch, MagicMock
from {module_path} import {agent_name}


class Test{agent_name}:
    """Unit test suite for {agent_name}."""

    @pytest.fixture
    def agent(self):
        """Create agent instance."""
        return {agent_name}()

    @pytest.fixture
    def sample_input(self):
        """Sample input data."""
        return {{
            "data": "test input",
            "source": "test"
        }}

    def test_initialization(self, agent):
        """Test agent initialization."""
        assert agent is not None
        assert hasattr(agent, 'execute')
        assert hasattr(agent, 'validate_input')
        assert hasattr(agent, 'validate_output')

    def test_execute_success(self, agent, sample_input):
        """Test successful execution."""
        result = agent.execute(sample_input)

        assert result is not None
        assert isinstance(result, dict)
        assert "status" in result
        assert result["status"] == "success"

    def test_execute_with_invalid_input(self, agent):
        """Test execution with invalid input."""
        invalid_input = {{}}

        result = agent.execute(invalid_input)

        assert result["status"] == "error"
        assert "error" in result

    def test_execute_with_missing_fields(self, agent):
        """Test execution with missing required fields."""
        incomplete_input = {{
            "partial": "data"
        }}

        result = agent.execute(incomplete_input)

        assert result["status"] == "error"

    def test_validate_input_valid(self, agent, sample_input):
        """Test input validation with valid data."""
        assert agent.validate_input(sample_input) is True

    def test_validate_input_invalid(self, agent):
        """Test input validation with invalid data."""
        invalid_data = {{}}

        assert agent.validate_input(invalid_data) is False

    def test_validate_output_valid(self, agent):
        """Test output validation with valid data."""
        valid_output = {{
            "status": "success",
            "result": {{"processed": True}}
        }}

        assert agent.validate_output(valid_output) is True

    def test_validate_output_invalid(self, agent):
        """Test output validation with invalid data."""
        invalid_output = {{}}

        # Depending on implementation, this might be True or False
        # Adjust based on your validation logic
        result = agent.validate_output(invalid_output)
        assert isinstance(result, bool)

    def test_batch_execute(self, agent):
        """Test batch execution."""
        batch_data = [
            {{"data": "input1"}},
            {{"data": "input2"}},
            {{"data": "input3"}}
        ]

        results = agent.batch_execute(batch_data)

        assert isinstance(results, list)
        assert len(results) == 3
        assert all(isinstance(r, dict) for r in results)

    def test_batch_execute_with_failures(self, agent):
        """Test batch execution with some failures."""
        batch_data = [
            {{"data": "valid"}},
            {{}},  # Invalid
            {{"data": "valid again"}}
        ]

        results = agent.batch_execute(batch_data)

        assert len(results) == 3
        # First and third should succeed, second should fail
        assert results[0]["status"] == "success"
        assert results[1]["status"] == "error"
        assert results[2]["status"] == "success"

    def test_error_handling(self, agent):
        """Test error handling."""
        # Test with data that causes an exception
        with patch.object(agent, '_process', side_effect=Exception("Test error")):
            result = agent.execute({{"data": "test"}})

            assert result["status"] == "error"
            assert "error" in result

    def test_logging(self, agent):
        """Test that logging occurs."""
        with patch.object(agent, 'logger') as mock_logger:
            agent.execute({{"data": "test"}})

            # Verify logging was called
            assert mock_logger.info.called or mock_logger.error.called

{'    @patch("shared.infrastructure.llm.ChatSession")' if agent_info['has_llm'] else ''}
{'    def test_llm_integration(self, mock_chat, agent, sample_input):' if agent_info['has_llm'] else ''}
{'        """Test LLM integration."""' if agent_info['has_llm'] else ''}
{'        mock_chat.return_value.chat.return_value.content = "LLM response"' if agent_info['has_llm'] else ''}
{'        ' if agent_info['has_llm'] else ''}
{'        result = agent.execute(sample_input)' if agent_info['has_llm'] else ''}
{'        ' if agent_info['has_llm'] else ''}
{'        assert result["status"] == "success"' if agent_info['has_llm'] else ''}

{'    def test_caching(self, agent, sample_input):' if agent_info['has_cache'] else ''}
{'        """Test caching behavior."""' if agent_info['has_cache'] else ''}
{'        # First call' if agent_info['has_cache'] else ''}
{'        result1 = agent.execute(sample_input)' if agent_info['has_cache'] else ''}
{'        ' if agent_info['has_cache'] else ''}
{'        # Second call (should use cache)' if agent_info['has_cache'] else ''}
{'        result2 = agent.execute(sample_input)' if agent_info['has_cache'] else ''}
{'        ' if agent_info['has_cache'] else ''}
{'        # Results should be the same' if agent_info['has_cache'] else ''}
{'        assert result1 == result2' if agent_info['has_cache'] else ''}


class Test{agent_name}Integration:
    """Integration tests for {agent_name}."""

    @pytest.fixture
    def agent(self):
        """Create agent instance."""
        return {agent_name}()

    def test_end_to_end_workflow(self, agent):
        """Test complete end-to-end workflow."""
        # Prepare realistic input
        input_data = {{
            "data": "real world data",
            "source": "integration test"
        }}

        # Execute
        result = agent.execute(input_data)

        # Verify
        assert result["status"] == "success"
        assert "result" in result

    def test_real_data_processing(self, agent):
        """Test with real-world data format."""
        # Use actual data format your agent will encounter
        real_data = {{
            "data": "sample production data"
        }}

        result = agent.execute(real_data)

        assert result["status"] == "success"

    def test_performance(self, agent):
        """Test performance with large dataset."""
        import time

        large_batch = [{{"data": f"item{{i}}"}} for i in range(100)]

        start_time = time.time()
        results = agent.batch_execute(large_batch)
        duration = time.time() - start_time

        assert len(results) == 100
        assert duration < 30  # Should complete in under 30 seconds

    def test_concurrent_execution(self, agent):
        """Test concurrent execution."""
        from concurrent.futures import ThreadPoolExecutor

        def execute_agent(data):
            return agent.execute({{"data": data}})

        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(execute_agent, f"data{{i}}") for i in range(10)]
            results = [f.result() for f in futures]

        assert len(results) == 10
        assert all(r["status"] == "success" for r in results)


# Fixtures for common test data
@pytest.fixture
def sample_csv_data():
    """Sample CSV data for testing."""
    return """name,value,date
Item1,100,2025-01-01
Item2,200,2025-01-02
Item3,300,2025-01-03"""


@pytest.fixture
def sample_json_data():
    """Sample JSON data for testing."""
    return {{
        "items": [
            {{"name": "Item1", "value": 100}},
            {{"name": "Item2", "value": 200}},
            {{"name": "Item3", "value": 300}}
        ]
    }}


@pytest.fixture
def mock_database():
    """Mock database for testing."""
    return MagicMock()


@pytest.fixture
def mock_cache():
    """Mock cache for testing."""
    return MagicMock()
'''

    @staticmethod
    def generate_conftest(agent_info: Dict[str, Any]) -> str:
        """Generate conftest.py with shared fixtures."""

        return '''"""
Shared test fixtures and configuration.

Auto-generated by GreenLang Test Generator
"""

import pytest
import os
import sys
from pathlib import Path


# Add app directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))


@pytest.fixture(scope="session")
def test_config():
    """Test configuration."""
    return {
        "environment": "test",
        "debug": True,
        "log_level": "DEBUG"
    }


@pytest.fixture(scope="function")
def temp_directory(tmp_path):
    """Create temporary directory for tests."""
    return tmp_path


@pytest.fixture(scope="function")
def sample_file(tmp_path):
    """Create a sample file for testing."""
    file_path = tmp_path / "sample.txt"
    file_path.write_text("Sample content for testing")
    return file_path


@pytest.fixture(autouse=True)
def reset_environment():
    """Reset environment between tests."""
    # Setup
    original_env = os.environ.copy()

    yield

    # Teardown
    os.environ.clear()
    os.environ.update(original_env)


def pytest_configure(config):
    """Pytest configuration."""
    config.addinivalue_line(
        "markers", "slow: marks tests as slow (deselect with '-m \"not slow\"')"
    )
    config.addinivalue_line(
        "markers", "integration: marks tests as integration tests"
    )
'''


def main():
    """CLI entry point."""
    parser = argparse.ArgumentParser(
        description='Generate tests for GreenLang agents',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Generate tests for an agent
  greenlang generate-tests app/agents/my_agent.py

  # Generate tests with custom output
  greenlang generate-tests app/agents/my_agent.py --output tests/test_my_agent.py

  # Generate tests for all agents
  greenlang generate-tests app/agents/*.py
        """
    )

    parser.add_argument('agent_file', help='Agent file to generate tests for')
    parser.add_argument('--output', help='Output test file')
    parser.add_argument('--conftest', action='store_true', help='Also generate conftest.py')

    args = parser.parse_args()

    if not os.path.exists(args.agent_file):
        print(f"Error: File not found: {args.agent_file}")
        sys.exit(1)

    print(f"\nAnalyzing agent: {args.agent_file}\n")

    # Analyze agent
    analyzer = AgentAnalyzer(args.agent_file)
    agent_info = analyzer.analyze()

    if not agent_info['agent_name']:
        print("Error: No agent class found in file")
        sys.exit(1)

    print(f"Agent: {agent_info['agent_name']}")
    print(f"Methods: {', '.join(agent_info['methods'])}")
    print(f"Features: LLM={agent_info['has_llm']}, Cache={agent_info['has_cache']}, DB={agent_info['has_database']}")

    # Generate tests
    print("\nGenerating tests...\n")

    generator = TestGenerator()
    test_code = generator.generate_unit_tests(agent_info, args.agent_file)

    # Determine output file
    if args.output:
        output_file = args.output
    else:
        agent_filename = Path(args.agent_file).stem
        output_file = f"tests/test_{agent_filename}.py"

    # Ensure tests directory exists
    os.makedirs(os.path.dirname(output_file), exist_ok=True)

    # Write test file
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(test_code)

    print(f"Generated: {output_file}")

    # Generate conftest if requested
    if args.conftest:
        conftest_code = generator.generate_conftest(agent_info)
        conftest_file = os.path.join(os.path.dirname(output_file), 'conftest.py')

        with open(conftest_file, 'w', encoding='utf-8') as f:
            f.write(conftest_code)

        print(f"Generated: {conftest_file}")

    print("\nTests generated successfully!")
    print(f"\nRun tests with:")
    print(f"  pytest {output_file} -v")
    print()


if __name__ == '__main__':
    main()
