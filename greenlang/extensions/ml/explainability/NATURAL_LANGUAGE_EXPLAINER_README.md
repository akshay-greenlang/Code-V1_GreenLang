# NaturalLanguageExplainer - Human-Readable Explanation Generator

## Overview

The `NaturalLanguageExplainer` class transforms technical machine learning model outputs (SHAP values, feature contributions) into clear, actionable natural language explanations tailored for different audiences and domains.

This module implements **zero-hallucination principles**:
- All explanations are derived from actual model outputs
- No fabricated insights or recommendations
- Complete provenance tracking for audit trails
- Multiple audience support (operator, engineer, executive, auditor)

## Key Features

### 1. Multi-Audience Explanations
Generate explanations optimized for different stakeholders:

- **Operator**: Simple, action-oriented language. Max 3 features. Focus on "what to do".
- **Engineer**: Technical precision. Max 5 features. Include calculations and standards.
- **Executive**: High-level summary. Max 2 features. Business-focused.
- **Auditor**: Comprehensive details. Max 10 features. Compliance-focused with provenance.

### 2. Multiple Decision Types
Support for different prediction scenarios:

- `FOULING_RISK`: Equipment fouling/scaling risk
- `EFFICIENCY_DEGRADATION`: Energy efficiency loss
- `MAINTENANCE_NEEDED`: Predictive maintenance indicators
- `EMISSIONS_HIGH`: Emissions/environmental concerns
- `ENERGY_WASTE`: Fuel/energy waste detection

### 3. Output Formats
Generate explanations in multiple formats:

- **Plain Text**: Simple, readable format
- **Markdown**: Structured with headers and lists
- **HTML**: Web-ready with styling support

### 4. Provenance Tracking
Complete audit trail with SHA-256 hashing:

```python
provenance_hash = "abc123def456..."  # SHA-256
timestamp = "2025-12-07T04:34:39.598854"
```

### 5. Multiple Languages
Extensible language support (currently English, framework for others).

## Installation

The module is part of GreenLang's ML explainability framework:

```python
from greenlang.ml.explainability import NaturalLanguageExplainer
from greenlang.ml.explainability import Audience, OutputFormat, DecisionType
```

## Quick Start

### Basic Usage

```python
from greenlang.ml.explainability import NaturalLanguageExplainer, Audience, DecisionType

# Create explainer for operators
explainer = NaturalLanguageExplainer(default_audience=Audience.OPERATOR)

# Generate explanation from model output
result = explainer.explain_prediction(
    prediction=0.85,                          # Model prediction (0-1)
    shap_values={
        "flue_gas_temperature": 0.35,
        "days_since_cleaning": 0.28,
        "excess_air": 0.12
    },
    feature_names={
        "flue_gas_temperature": "Flue Gas Temperature",
        "days_since_cleaning": "Days Since Cleaning",
        "excess_air": "Excess Air %"
    },
    feature_values={
        "flue_gas_temperature": 485.2,
        "days_since_cleaning": 120,
        "excess_air": 22.5
    },
    decision_type=DecisionType.FOULING_RISK,
    confidence=0.88
)

print(result.text_summary)
# Output:
# Boiler Fouling Alert
# ====================
# Prediction: 85.0%
# Confidence: 88%
#
# Key Factors:
#   1. Flue Gas Temperature: 485.20 (impact: 35.0%)
#   2. Days Since Cleaning: 120.00 (impact: 28.0%)
#   3. Excess Air %: 22.50 (impact: 12.0%)

# Get recommendations
for rec in result.recommendations:
    print(f"- {rec}")
# - Schedule equipment cleaning or maintenance
# - Inspect for deposit buildup
# - Monitor fouling indicators closely
```

### Factory Function

```python
from greenlang.ml.explainability import create_natural_language_explainer

# Quick creation with predefined audiences
explainer = create_natural_language_explainer(
    audience='operator',      # 'operator', 'engineer', 'executive', 'auditor'
    output_format='markdown'  # 'text', 'markdown', 'html'
)
```

## API Reference

### NaturalLanguageExplainer Class

#### Initialization

```python
explainer = NaturalLanguageExplainer(
    default_audience: Audience = Audience.ENGINEER,
    default_format: OutputFormat = OutputFormat.PLAIN_TEXT,
    include_provenance: bool = True
)
```

#### Methods

##### `explain_prediction()`

Generate explanation for a model prediction.

```python
result = explainer.explain_prediction(
    prediction: float,                           # Model output (0-1)
    shap_values: Dict[str, float],              # Feature contributions
    feature_names: Dict[str, str],              # Readable feature names
    decision_type: DecisionType = DecisionType.FOULING_RISK,
    audience: Optional[Audience] = None,         # Override default
    confidence: float = 0.85,                    # Prediction confidence
    feature_values: Optional[Dict[str, float]] = None,  # Actual values
    baseline: Optional[float] = None             # Comparison baseline
) -> ExplanationOutput
```

**Returns**: `ExplanationOutput` containing:
- `text_summary`: Plain text explanation
- `markdown_summary`: Markdown formatted explanation
- `html_summary`: HTML formatted explanation
- `confidence`: Explanation confidence score
- `provenance_hash`: SHA-256 audit hash
- `top_factors`: Top 3-10 contributing features
- `recommendations`: Actionable recommendations
- `audience`: Target audience
- `metadata`: Additional context

##### `explain_decision()`

Generate explanation for a decision with structured factors.

```python
explanation = explainer.explain_decision(
    decision_type: DecisionType,
    factors: Dict[str, Any],     # Structured factors
    confidence: float,            # Decision confidence
    audience: Optional[Audience] = None
) -> str
```

##### `generate_summary()`

Combine multiple explanations into a unified summary.

```python
summary = explainer.generate_summary(
    explanations: List[ExplanationOutput],
    audience: Optional[Audience] = None,
    output_format: OutputFormat = OutputFormat.PLAIN_TEXT
) -> str
```

##### `translate_to_language()`

Translate explanation to different language (extensible).

```python
translated = explainer.translate_to_language(
    explanation: str,
    language: str = "en"  # Currently supports 'en'
) -> str
```

### Data Models

#### Audience Enum

```python
class Audience(str, Enum):
    OPERATOR = "operator"       # Equipment operators
    ENGINEER = "engineer"       # Process engineers
    EXECUTIVE = "executive"     # C-suite executives
    AUDITOR = "auditor"         # Regulatory auditors
```

#### DecisionType Enum

```python
class DecisionType(str, Enum):
    FOULING_RISK = "fouling_risk"
    EFFICIENCY_DEGRADATION = "efficiency_degradation"
    MAINTENANCE_NEEDED = "maintenance_needed"
    EMISSIONS_HIGH = "emissions_high"
    ENERGY_WASTE = "energy_waste"
```

#### ExplanationOutput Model

```python
class ExplanationOutput(BaseModel):
    text_summary: str                              # Plain text
    markdown_summary: str                          # Markdown
    html_summary: str                              # HTML
    confidence: float                              # 0-1 score
    provenance_hash: str                           # SHA-256
    top_factors: List[Tuple[str, float]]          # (feature, contribution)
    recommendations: List[str]                     # Action items
    audience: str                                  # Target audience
    timestamp: datetime                            # Generation time
    metadata: Dict[str, Any]                       # Additional context
```

## Audience Configuration

Each audience receives customized explanations:

### Operator (Equipment Operator)
```python
{
    "complexity": "simple",
    "max_factors": 3,
    "include_technical": False,
    "include_numbers": True,
    "include_recommendations": True,
    "language_style": "action-oriented"
}
```

Example output:
```
EQUIPMENT ALERT
The boiler shows HIGH fouling risk (85%).

Main Issues:
  1. Flue gas temperature is too high
  2. Not cleaned in 120 days

Actions Needed:
  - Schedule cleaning this week
  - Check combustion settings
```

### Engineer (Process Engineer)
```python
{
    "complexity": "technical",
    "max_factors": 5,
    "include_technical": True,
    "include_numbers": True,
    "include_recommendations": True,
    "language_style": "precise"
}
```

Example output:
```
FOULING RISK ANALYSIS
Prediction: 85%, Confidence: 92%

Contributing Factors:
  1. Flue Gas Temperature: 485.2F (35% impact)
  2. Days Since Cleaning: 120 days (28% impact)
  3. Excess Air: 22.5% (12% impact)
  4. Stack Temperature: 325.0F (8% impact)
  5. Pressure Drop: 1.5 PSI (2% impact)

Technical Assessment: High fouling probability
per ASME PTC-4 standards. Recommended cleaning
interval exceeded by 45 days.
```

### Executive (C-Suite)
```python
{
    "complexity": "minimal",
    "max_factors": 2,
    "include_technical": False,
    "include_numbers": True,
    "include_recommendations": False,
    "language_style": "business-oriented"
}
```

Example output:
```
BOILER STATUS
Risk: HIGH (85%)
Primary Issue: Heat transfer degradation

Expected Impact: 5-8% efficiency loss
Estimated Cost: $12,000-18,000/year
```

### Auditor (Regulatory Auditor)
```python
{
    "complexity": "comprehensive",
    "max_factors": 10,
    "include_technical": True,
    "include_numbers": True,
    "include_recommendations": False,
    "language_style": "compliance-focused"
}
```

Example output includes all features, complete calculations, standards references, and provenance hash.

## Real-World Examples

### Example 1: Operator Alert

```python
explainer = create_natural_language_explainer(audience='operator')

result = explainer.explain_prediction(
    prediction=0.87,
    shap_values={"flue_gas_temp": 0.38, "days_clean": 0.32},
    feature_names={"flue_gas_temp": "Flue Gas Temperature",
                   "days_clean": "Days Since Cleaning"},
    feature_values={"flue_gas_temp": 510.0, "days_clean": 145},
    decision_type=DecisionType.FOULING_RISK
)

print(result.text_summary)
print("\nDo This:")
for rec in result.recommendations:
    print(f"  â€¢ {rec}")
```

### Example 2: Engineering Analysis

```python
explainer = create_natural_language_explainer(audience='engineer')

result = explainer.explain_prediction(
    prediction=0.75,
    shap_values={
        "combustion_efficiency": -0.12,
        "stack_temperature": 0.22,
        "excess_air": 0.18
    },
    feature_names={
        "combustion_efficiency": "Combustion Efficiency (ASME PTC-4)",
        "stack_temperature": "Stack Temperature (F)",
        "excess_air": "Excess Air (%)"
    },
    feature_values={
        "combustion_efficiency": 0.82,
        "stack_temperature": 385.0,
        "excess_air": 25.5
    },
    decision_type=DecisionType.EFFICIENCY_DEGRADATION,
    baseline=0.88
)

print(result.markdown_summary)
```

### Example 3: Multi-Model Summary

```python
explainer = NaturalLanguageExplainer()

# Multiple model outputs
fouling_result = explainer.explain_prediction(...)
efficiency_result = explainer.explain_prediction(...)
maintenance_result = explainer.explain_prediction(...)

# Unified summary for plant manager
summary = explainer.generate_summary(
    [fouling_result, efficiency_result, maintenance_result],
    audience=Audience.EXECUTIVE
)

print(summary)
```

## Testing

The module includes 33 comprehensive unit tests:

```bash
pytest tests/unit/test_natural_language_explainer.py -v

# Results: 33 passed in 1.01s
```

Test coverage includes:
- All audience types
- All decision types
- All output formats
- Edge cases (zero contribution, critical risk, many features)
- Provenance hash generation
- Serialization/deserialization

## Principles

### 1. Zero Hallucination
All explanations derive from actual model outputs:
- No invented insights
- No fabricated recommendations
- All facts traceable to input data

### 2. Audience Adaptation
Language and detail level match the audience's needs:
- Operators: Action-oriented, simple language
- Engineers: Technical precision, standards references
- Executives: Business impact, brief summaries
- Auditors: Complete documentation, full traceability

### 3. Provenance
Complete audit trail for every explanation:
- SHA-256 hash of all inputs
- Timestamp of generation
- Source model/features
- Explainability method

### 4. Clarity First
Prioritize clarity over technical completeness:
- Short sentences
- Concrete examples
- Actionable guidance
- No jargon (unless required)

## Integration with Other Components

### With SHAP Explainer
```python
from greenlang.ml.explainability import ShapExplainer, NaturalLanguageExplainer

shap_explainer = ShapExplainer(model)
shap_result = shap_explainer.explain(instance)

nl_explainer = NaturalLanguageExplainer(audience=Audience.OPERATOR)
explanation = nl_explainer.explain_prediction(
    prediction=shap_result.prediction,
    shap_values=shap_result.feature_contributions,
    feature_names=shap_result.feature_names
)
```

### With LIME Explainer
```python
from greenlang.ml.explainability import LIMEExplainer, NaturalLanguageExplainer

lime_explainer = LIMEExplainer(model)
lime_result = lime_explainer.explain(instance)

nl_explainer = NaturalLanguageExplainer()
explanation = nl_explainer.explain_prediction(
    prediction=lime_result.prediction,
    shap_values=lime_result.feature_weights
)
```

## Performance

- **Generation time**: 5-50ms per explanation
- **Memory usage**: ~2MB for typical instance
- **Scalability**: Can handle 1000+ explanations/minute

## Future Enhancements

1. **Multi-language support**: Spanish, French, German, Mandarin
2. **Custom templates**: User-defined explanation templates
3. **Interactive explanations**: Drill-down into factors
4. **Visualization integration**: Charts and graphs
5. **Accessibility**: WCAG AAA compliance for HTML output
6. **Domain extensions**: Industry-specific templates

## Contributing

To add new audience types or decision types:

1. Add enum value to `Audience` or `DecisionType`
2. Add audience configuration to `AUDIENCE_CONFIG`
3. Add templates to `TEMPLATES` dictionary
4. Write tests for new type
5. Document in this README

## License

Part of GreenLang - Regulatory Compliance & Emissions Management Platform

## Support

For issues, questions, or suggestions:
- Documentation: See `docs/explainability/`
- Examples: See `docs/examples/natural_language_explainer_examples.py`
- Tests: See `tests/unit/test_natural_language_explainer.py`
