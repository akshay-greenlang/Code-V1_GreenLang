"""
GL-007 FurnacePulse - Guardrails Integration
============================================================

Provides safety guardrails integration for the agent including:
- Input validation and sanitization
- Output checking for data leakage
- Action gating with velocity limits
- Provenance tracking for audit compliance

Generated by GreenLang Enhancement Script
"""

import hashlib
import json
import logging
import time
from dataclasses import dataclass, field
from datetime import datetime, timezone
from enum import Enum, auto
from functools import wraps
from typing import Any, Callable, Dict, List, Optional, TypeVar
import uuid

logger = logging.getLogger(__name__)

# Agent configuration
AGENT_ID = "GL-007"
AGENT_NAME = "FurnacePulse"


class ViolationSeverity(Enum):
    """Severity levels for guardrail violations."""
    INFO = auto()
    WARNING = auto()
    ERROR = auto()
    CRITICAL = auto()
    BLOCKING = auto()


class GuardrailProfile(Enum):
    """Predefined guardrail profiles."""
    MINIMAL = auto()      # Basic input validation
    STANDARD = auto()     # Input + output validation
    STRICT = auto()       # Full validation + action gating
    INDUSTRIAL = auto()   # Strict + physical safety
    REGULATORY = auto()   # Full compliance mode


@dataclass
class GuardrailViolation:
    """Record of a guardrail violation."""
    violation_id: str
    guardrail_name: str
    severity: ViolationSeverity
    message: str
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    context: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "violation_id": self.violation_id,
            "guardrail_name": self.guardrail_name,
            "severity": self.severity.name,
            "message": self.message,
            "timestamp": self.timestamp.isoformat(),
            "context": self.context,
        }


@dataclass
class GuardrailResult:
    """Result of guardrail checks."""
    passed: bool
    violations: List[GuardrailViolation] = field(default_factory=list)
    execution_time_ms: float = 0.0

    @property
    def has_blocking_violation(self) -> bool:
        return any(v.severity == ViolationSeverity.BLOCKING for v in self.violations)


class GuardrailsIntegration:
    """
    Main guardrails integration class for GL-007.

    Provides input validation, output checking, action gating,
    and provenance tracking for all agent operations.
    """

    def __init__(
        self,
        profile: GuardrailProfile = GuardrailProfile.INDUSTRIAL,
        max_actions_per_minute: int = 60,
    ):
        self.profile = profile
        self.max_actions_per_minute = max_actions_per_minute
        self.action_timestamps: List[float] = []
        self.violation_log: List[GuardrailViolation] = []

        logger.info(f"Initialized guardrails for {AGENT_ID} with profile {profile.name}")

    def check_input(self, input_data: Any, context: Optional[Dict] = None) -> GuardrailResult:
        """
        Validate input data.

        Args:
            input_data: Input to validate
            context: Additional context

        Returns:
            GuardrailResult with any violations
        """
        start = time.time()
        violations = []

        # Check for prompt injection patterns
        if isinstance(input_data, str):
            injection_patterns = [
                "ignore previous instructions",
                "disregard",
                "override",
                "system prompt",
                "jailbreak",
            ]
            input_lower = input_data.lower()
            for pattern in injection_patterns:
                if pattern in input_lower:
                    violations.append(GuardrailViolation(
                        violation_id=str(uuid.uuid4()),
                        guardrail_name="PromptInjection",
                        severity=ViolationSeverity.BLOCKING,
                        message=f"Potential prompt injection detected: {pattern}",
                        context={"input_hash": self._hash(input_data)},
                    ))

        # Check for physical bounds (for industrial profile)
        if self.profile in (GuardrailProfile.INDUSTRIAL, GuardrailProfile.REGULATORY):
            violations.extend(self._check_physical_bounds(input_data))

        elapsed = (time.time() - start) * 1000
        passed = not any(v.severity == ViolationSeverity.BLOCKING for v in violations)

        return GuardrailResult(passed=passed, violations=violations, execution_time_ms=elapsed)

    def check_output(self, output_data: Any, context: Optional[Dict] = None) -> GuardrailResult:
        """
        Check output data for leakage or policy violations.

        Args:
            output_data: Output to check
            context: Additional context

        Returns:
            GuardrailResult with any violations
        """
        start = time.time()
        violations = []

        # Check for PII/sensitive data patterns
        if isinstance(output_data, str):
            sensitive_patterns = [
                ("api_key", r"api[_-]?key\s*[=:]"),
                ("password", r"password\s*[=:]"),
                ("token", r"token\s*[=:]"),
                ("secret", r"secret\s*[=:]"),
            ]
            import re
            for name, pattern in sensitive_patterns:
                if re.search(pattern, output_data, re.IGNORECASE):
                    violations.append(GuardrailViolation(
                        violation_id=str(uuid.uuid4()),
                        guardrail_name="DataLeakage",
                        severity=ViolationSeverity.CRITICAL,
                        message=f"Potential sensitive data in output: {name}",
                    ))

        elapsed = (time.time() - start) * 1000
        passed = not any(v.severity == ViolationSeverity.BLOCKING for v in violations)

        return GuardrailResult(passed=passed, violations=violations, execution_time_ms=elapsed)

    def check_action(self, action_data: Any, context: Optional[Dict] = None) -> GuardrailResult:
        """
        Gate action execution with rate limiting.

        Args:
            action_data: Action to check
            context: Additional context with action_type

        Returns:
            GuardrailResult with any violations
        """
        start = time.time()
        violations = []

        # Rate limiting
        now = time.time()
        self.action_timestamps = [t for t in self.action_timestamps if now - t < 60]

        if len(self.action_timestamps) >= self.max_actions_per_minute:
            violations.append(GuardrailViolation(
                violation_id=str(uuid.uuid4()),
                guardrail_name="ActionGate",
                severity=ViolationSeverity.BLOCKING,
                message=f"Rate limit exceeded: {self.max_actions_per_minute}/min",
            ))
        else:
            self.action_timestamps.append(now)

        elapsed = (time.time() - start) * 1000
        passed = not any(v.severity == ViolationSeverity.BLOCKING for v in violations)

        return GuardrailResult(passed=passed, violations=violations, execution_time_ms=elapsed)

    def _check_physical_bounds(self, data: Any) -> List[GuardrailViolation]:
        """Check physical safety constraints."""
        violations = []

        if isinstance(data, dict):
            bounds = {
                "temperature": (0, 2000),      # Kelvin
                "pressure": (0, 100e6),        # Pascals
                "flow_rate": (0, 10000),       # kg/s
                "efficiency": (0, 1.0),        # fraction
            }

            for key, (min_val, max_val) in bounds.items():
                if key in data:
                    value = data[key]
                    if isinstance(value, (int, float)):
                        if value < min_val or value > max_val:
                            violations.append(GuardrailViolation(
                                violation_id=str(uuid.uuid4()),
                                guardrail_name="SafetyEnvelope",
                                severity=ViolationSeverity.WARNING,
                                message=f"{key}={value} outside bounds [{min_val}, {max_val}]",
                            ))

        return violations

    def _hash(self, data: Any) -> str:
        """Compute SHA-256 hash of data."""
        try:
            json_str = json.dumps(data, sort_keys=True, default=str)
        except (TypeError, ValueError):
            json_str = str(data)
        return hashlib.sha256(json_str.encode()).hexdigest()[:16]


# Global instance
_guardrails: Optional[GuardrailsIntegration] = None


def get_guardrails(
    profile: GuardrailProfile = GuardrailProfile.INDUSTRIAL
) -> GuardrailsIntegration:
    """Get or create the global guardrails instance."""
    global _guardrails
    if _guardrails is None:
        _guardrails = GuardrailsIntegration(profile=profile)
    return _guardrails


T = TypeVar('T')


def with_guardrails(
    profile: GuardrailProfile = GuardrailProfile.INDUSTRIAL,
) -> Callable:
    """
    Decorator for wrapping functions with guardrail protection.

    Example:
        @with_guardrails()
        def calculate_efficiency(data: dict) -> dict:
            return {"efficiency": 0.85}
    """
    def decorator(func: Callable[..., T]) -> Callable[..., T]:
        @wraps(func)
        def wrapper(*args, **kwargs) -> T:
            guardrails = get_guardrails(profile)

            # Check input
            input_data = {"args": args, "kwargs": kwargs}
            input_result = guardrails.check_input(input_data)

            if input_result.has_blocking_violation:
                raise ValueError(
                    f"Input blocked: {input_result.violations[0].message}"
                )

            # Execute function
            result = func(*args, **kwargs)

            # Check output
            output_result = guardrails.check_output(result)

            if output_result.has_blocking_violation:
                raise ValueError(
                    f"Output blocked: {output_result.violations[0].message}"
                )

            return result

        return wrapper
    return decorator


__all__ = [
    "GuardrailsIntegration",
    "GuardrailProfile",
    "GuardrailResult",
    "GuardrailViolation",
    "ViolationSeverity",
    "get_guardrails",
    "with_guardrails",
]
