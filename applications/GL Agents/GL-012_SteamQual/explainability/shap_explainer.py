"""
GL-012 SteamQual - SHAP Explainability Module
=================================================================

Provides SHAP-based explanations for model predictions.

Features:
- TreeExplainer for tree-based models
- KernelExplainer for any model
- Feature importance calculation
- Provenance tracking for explanations

Generated by GreenLang Enhancement Script
"""

import hashlib
import json
import logging
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Union

import numpy as np

logger = logging.getLogger(__name__)

# Agent configuration
AGENT_ID = "GL-012"
AGENT_NAME = "SteamQual"

# Try to import SHAP
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    logger.warning("SHAP not installed. Using fallback explanations.")


@dataclass
class FeatureImportance:
    """Feature importance with SHAP values."""
    feature_name: str
    shap_value: float
    contribution_percent: float
    direction: str  # "positive" or "negative"


@dataclass
class Explanation:
    """Complete explanation for a prediction."""
    explanation_id: str
    agent_id: str = AGENT_ID
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    base_value: float = 0.0
    predicted_value: float = 0.0
    feature_importances: List[FeatureImportance] = field(default_factory=list)
    provenance_hash: str = ""

    def to_dict(self) -> Dict[str, Any]:
        return {
            "explanation_id": self.explanation_id,
            "agent_id": self.agent_id,
            "timestamp": self.timestamp.isoformat(),
            "base_value": self.base_value,
            "predicted_value": self.predicted_value,
            "feature_importances": [
                {
                    "feature": f.feature_name,
                    "shap_value": f.shap_value,
                    "contribution_percent": f.contribution_percent,
                    "direction": f.direction,
                }
                for f in self.feature_importances
            ],
            "provenance_hash": self.provenance_hash,
        }


class SHAPExplainer:
    """
    SHAP-based explainer for GL-012.

    Provides explanations for model predictions using SHAP values
    with fallback for environments without SHAP.
    """

    def __init__(
        self,
        model: Any = None,
        feature_names: Optional[List[str]] = None,
        explainer_type: str = "auto"
    ):
        """
        Initialize the explainer.

        Args:
            model: The model to explain
            feature_names: Names of input features
            explainer_type: "tree", "kernel", or "auto"
        """
        self.model = model
        self.feature_names = feature_names or []
        self.explainer_type = explainer_type
        self.explainer = None
        self.explanation_cache: Dict[str, Explanation] = {}

        if model is not None and SHAP_AVAILABLE:
            self._init_explainer()

    def _init_explainer(self):
        """Initialize the appropriate SHAP explainer."""
        if not SHAP_AVAILABLE or self.model is None:
            return

        try:
            if self.explainer_type == "tree":
                self.explainer = shap.TreeExplainer(self.model)
            elif self.explainer_type == "kernel":
                # Kernel explainer needs background data
                background = np.zeros((1, len(self.feature_names)))
                self.explainer = shap.KernelExplainer(
                    self.model.predict,
                    background
                )
            else:
                # Auto-detect
                try:
                    self.explainer = shap.TreeExplainer(self.model)
                except Exception:
                    background = np.zeros((1, len(self.feature_names)))
                    self.explainer = shap.KernelExplainer(
                        self.model.predict,
                        background
                    )

            logger.info(f"Initialized {type(self.explainer).__name__} for {AGENT_ID}")
        except Exception as e:
            logger.warning(f"Failed to initialize SHAP explainer: {e}")
            self.explainer = None

    def explain(
        self,
        inputs: Union[np.ndarray, List[List[float]], Dict[str, float]],
        prediction: Optional[float] = None,
    ) -> Explanation:
        """
        Generate explanation for inputs.

        Args:
            inputs: Input features (array, list, or dict)
            prediction: The model's prediction (optional)

        Returns:
            Explanation with feature importances
        """
        import uuid

        # Convert inputs to array
        if isinstance(inputs, dict):
            input_array = np.array([[inputs.get(f, 0) for f in self.feature_names]])
        elif isinstance(inputs, list):
            input_array = np.array(inputs).reshape(1, -1)
        else:
            input_array = inputs.reshape(1, -1) if inputs.ndim == 1 else inputs

        explanation_id = str(uuid.uuid4())

        if SHAP_AVAILABLE and self.explainer is not None:
            return self._shap_explain(input_array, prediction, explanation_id)
        else:
            return self._fallback_explain(input_array, prediction, explanation_id)

    def _shap_explain(
        self,
        inputs: np.ndarray,
        prediction: Optional[float],
        explanation_id: str
    ) -> Explanation:
        """Generate SHAP-based explanation."""
        shap_values = self.explainer.shap_values(inputs)

        # Handle different SHAP value formats
        if isinstance(shap_values, list):
            shap_values = shap_values[0]

        shap_array = shap_values[0] if shap_values.ndim > 1 else shap_values

        # Get base value
        base_value = float(self.explainer.expected_value)
        if isinstance(self.explainer.expected_value, np.ndarray):
            base_value = float(self.explainer.expected_value[0])

        # Calculate feature importances
        total_abs = np.abs(shap_array).sum()
        feature_importances = []

        for i, shap_val in enumerate(shap_array):
            name = self.feature_names[i] if i < len(self.feature_names) else f"feature_{i}"
            contribution = (abs(shap_val) / total_abs * 100) if total_abs > 0 else 0

            feature_importances.append(FeatureImportance(
                feature_name=name,
                shap_value=float(shap_val),
                contribution_percent=float(contribution),
                direction="positive" if shap_val > 0 else "negative",
            ))

        # Sort by absolute contribution
        feature_importances.sort(key=lambda x: abs(x.shap_value), reverse=True)

        # Compute provenance hash
        provenance_data = {
            "inputs": inputs.tolist(),
            "shap_values": shap_array.tolist(),
            "base_value": base_value,
        }
        provenance_hash = hashlib.sha256(
            json.dumps(provenance_data, sort_keys=True).encode()
        ).hexdigest()

        predicted_value = prediction if prediction is not None else (base_value + shap_array.sum())

        return Explanation(
            explanation_id=explanation_id,
            base_value=base_value,
            predicted_value=float(predicted_value),
            feature_importances=feature_importances,
            provenance_hash=provenance_hash,
        )

    def _fallback_explain(
        self,
        inputs: np.ndarray,
        prediction: Optional[float],
        explanation_id: str
    ) -> Explanation:
        """Generate fallback explanation (feature magnitude-based)."""
        # Use input magnitudes as proxy for importance
        input_array = inputs[0] if inputs.ndim > 1 else inputs
        total_abs = np.abs(input_array).sum()

        feature_importances = []
        for i, value in enumerate(input_array):
            name = self.feature_names[i] if i < len(self.feature_names) else f"feature_{i}"
            contribution = (abs(value) / total_abs * 100) if total_abs > 0 else 0

            feature_importances.append(FeatureImportance(
                feature_name=name,
                shap_value=float(value),  # Using value as proxy
                contribution_percent=float(contribution),
                direction="positive" if value > 0 else "negative",
            ))

        feature_importances.sort(key=lambda x: abs(x.shap_value), reverse=True)

        provenance_hash = hashlib.sha256(
            json.dumps(inputs.tolist(), sort_keys=True).encode()
        ).hexdigest()

        return Explanation(
            explanation_id=explanation_id,
            base_value=0.0,
            predicted_value=prediction or 0.0,
            feature_importances=feature_importances,
            provenance_hash=provenance_hash,
        )

    def get_top_features(
        self,
        explanation: Explanation,
        n: int = 5
    ) -> List[FeatureImportance]:
        """Get top N most important features."""
        return explanation.feature_importances[:n]

    def generate_text_explanation(
        self,
        explanation: Explanation,
        top_n: int = 3
    ) -> str:
        """Generate human-readable explanation text."""
        top_features = self.get_top_features(explanation, top_n)

        lines = [
            f"Prediction: {explanation.predicted_value:.4f}",
            f"Base value: {explanation.base_value:.4f}",
            "",
            "Top contributing factors:",
        ]

        for i, f in enumerate(top_features, 1):
            direction = "increases" if f.direction == "positive" else "decreases"
            lines.append(
                f"  {i}. {f.feature_name} {direction} prediction by "
                f"{abs(f.shap_value):.4f} ({f.contribution_percent:.1f}%)"
            )

        return "\n".join(lines)


# Global explainer instance
_explainer: Optional[SHAPExplainer] = None


def get_explainer(
    model: Any = None,
    feature_names: Optional[List[str]] = None
) -> SHAPExplainer:
    """Get or create the global explainer instance."""
    global _explainer
    if _explainer is None or model is not None:
        _explainer = SHAPExplainer(model=model, feature_names=feature_names)
    return _explainer


__all__ = [
    "SHAPExplainer",
    "Explanation",
    "FeatureImportance",
    "get_explainer",
]
