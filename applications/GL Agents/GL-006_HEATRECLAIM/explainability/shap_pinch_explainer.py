"""
GL-006 HEATRECLAIM - SHAP Pinch Analysis Explainer

Implements SHAP TreeExplainer for pinch analysis feature attribution,
explaining which stream properties and operating conditions drive
minimum utility requirements, pinch temperature location, and
heat recovery potential.

Reference:
    - Lundberg & Lee, "A Unified Approach to Interpreting Model Predictions", NeurIPS 2017
    - Linnhoff & Hindmarsh, "The Pinch Design Method for Heat Exchanger Networks", 1983

Zero-Hallucination: All SHAP values are computed deterministically using
TreeExplainer on trained surrogate models, never generated by LLMs.

Example:
    >>> explainer = SHAPPinchExplainer(model_version="1.0.0")
    >>> result = explainer.explain_pinch_result(
    ...     pinch_result, hot_streams, cold_streams
    ... )
    >>> print(f"Top driver: {result.top_drivers[0].feature_name}")
    >>> print(f"Impact: {result.top_drivers[0].direction}")
"""

from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Tuple, Union
from enum import Enum
import hashlib
import json
import logging
import math

import numpy as np

try:
    import shap
    HAS_SHAP = True
except ImportError:
    HAS_SHAP = False

try:
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.preprocessing import StandardScaler
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False

from ..core.schemas import (
    HeatStream,
    PinchAnalysisResult,
    HENDesign,
    FeatureAttribution,
    ExplainabilityReport,
)

logger = logging.getLogger(__name__)


class PinchOutputType(Enum):
    """Type of pinch analysis output being explained."""

    HOT_UTILITY = "hot_utility"
    COLD_UTILITY = "cold_utility"
    PINCH_TEMPERATURE = "pinch_temperature"
    HEAT_RECOVERY = "heat_recovery"
    EXCHANGER_COUNT = "exchanger_count"
    TOTAL_AREA = "total_area"
    CAPITAL_COST = "capital_cost"
    PAYBACK_PERIOD = "payback_period"


class StreamFeatureCategory(Enum):
    """Category of stream-based features."""

    TEMPERATURE = "temperature"
    FLOW_RATE = "flow_rate"
    HEAT_CAPACITY = "heat_capacity"
    DUTY = "duty"
    APPROACH_TEMP = "approach_temperature"
    ECONOMIC = "economic"
    CONSTRAINT = "constraint"


@dataclass
class PinchDriverInfo:
    """
    Information about a top driver of pinch analysis results.

    Attributes:
        feature_name: Name of the driving feature
        shap_value: SHAP value contribution
        feature_value: Actual value of the feature
        direction: "increases" or "decreases" the target
        category: Feature category for grouping
        rank: Importance ranking (1 = most important)
        contribution_percent: Percentage of total explanation
        stream_id: Associated stream ID (if applicable)
        engineering_interpretation: Domain-specific explanation
        sensitivity: Sensitivity coefficient (change in output per unit input change)
    """

    feature_name: str
    shap_value: float
    feature_value: float
    direction: str
    category: StreamFeatureCategory
    rank: int
    contribution_percent: float
    stream_id: Optional[str] = None
    engineering_interpretation: str = ""
    sensitivity: float = 0.0


@dataclass
class SHAPPinchResult:
    """
    Result from SHAP analysis of pinch analysis calculation.

    Attributes:
        output_type: Type of pinch output explained
        calculation_id: Reference to original pinch calculation
        feature_names: Ordered list of feature names
        shap_values: SHAP values for each feature
        base_value: Expected value (model baseline)
        feature_values: Input feature values
        predicted_value: Predicted output value
        actual_value: Actual output value from calculation
        feature_importance: Aggregated feature importance
        top_drivers: Ranked list of top drivers
        interaction_effects: Pairwise interaction effects
        hot_stream_contributions: Contributions by hot stream
        cold_stream_contributions: Contributions by cold stream
        delta_t_min_sensitivity: Sensitivity to delta T min
        timestamp: Analysis timestamp
        computation_hash: SHA-256 hash for provenance
        model_version: Version of explainer model
    """

    output_type: PinchOutputType
    calculation_id: str
    feature_names: List[str]
    shap_values: np.ndarray
    base_value: float
    feature_values: np.ndarray
    predicted_value: float
    actual_value: float
    feature_importance: Dict[str, float]
    top_drivers: List[PinchDriverInfo]
    interaction_effects: Dict[str, Dict[str, float]]
    hot_stream_contributions: Dict[str, float]
    cold_stream_contributions: Dict[str, float]
    delta_t_min_sensitivity: float
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    computation_hash: str = ""
    model_version: str = ""


@dataclass
class GlobalPinchExplanation:
    """
    Global explanation aggregated across multiple pinch analyses.

    Attributes:
        output_type: Type of pinch output explained
        model_version: Version of explainer model
        num_analyses: Number of analyses included
        mean_abs_shap: Mean absolute SHAP by feature
        feature_ranking: Features ranked by importance
        direction_consistency: How consistent is feature direction (0-1)
        typical_ranges: Typical value ranges for key features
        sensitivity_matrix: Sensitivity of outputs to each input
        timestamp: Summary timestamp
        computation_hash: SHA-256 hash for provenance
    """

    output_type: PinchOutputType
    model_version: str
    num_analyses: int
    mean_abs_shap: Dict[str, float]
    feature_ranking: List[str]
    direction_consistency: Dict[str, float]
    typical_ranges: Dict[str, Tuple[float, float]]
    sensitivity_matrix: Dict[str, Dict[str, float]]
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    computation_hash: str = ""


class SHAPPinchExplainer:
    """
    SHAP-based explainer for pinch analysis results.

    Uses TreeExplainer on surrogate models to attribute pinch analysis
    outputs (minimum utilities, pinch temperature, heat recovery) to
    input features (stream temperatures, flow rates, heat capacities).

    Key Features:
        - Local explanations for individual pinch analyses
        - Global summaries across multiple analyses
        - Stream-level contribution attribution
        - Delta T min sensitivity analysis
        - Engineering-relevant interpretations

    Reference:
        ASME PTC 4.4 for heat recovery efficiency standards
        IChemE User Guide on Process Integration (Linnhoff et al.)

    Example:
        >>> explainer = SHAPPinchExplainer(model_version="1.0.0")
        >>> result = explainer.explain_hot_utility(
        ...     pinch_result, hot_streams, cold_streams
        ... )
        >>> for driver in result.top_drivers[:5]:
        ...     print(f"{driver.feature_name}: {driver.contribution_percent:.1f}%")
    """

    VERSION = "1.0.0"

    # Engineering interpretations for pinch-relevant features
    ENGINEERING_INTERPRETATIONS = {
        "T_supply": "Supply temperature determines heat available for exchange",
        "T_target": "Target temperature defines required heating/cooling",
        "FCp": "Heat capacity rate determines heat transfer rate capability",
        "duty": "Stream duty represents total heat load",
        "delta_T": "Temperature difference affects heat exchanger size",
        "delta_t_min": "Minimum approach temperature constrains recovery",
        "m_dot": "Mass flow rate affects heat capacity rate",
        "Cp": "Specific heat capacity affects energy per unit mass",
    }

    # Feature category mappings
    FEATURE_CATEGORIES = {
        "T_supply": StreamFeatureCategory.TEMPERATURE,
        "T_target": StreamFeatureCategory.TEMPERATURE,
        "T_": StreamFeatureCategory.TEMPERATURE,
        "m_dot": StreamFeatureCategory.FLOW_RATE,
        "flow": StreamFeatureCategory.FLOW_RATE,
        "Cp": StreamFeatureCategory.HEAT_CAPACITY,
        "FCp": StreamFeatureCategory.HEAT_CAPACITY,
        "duty": StreamFeatureCategory.DUTY,
        "delta": StreamFeatureCategory.APPROACH_TEMP,
        "cost": StreamFeatureCategory.ECONOMIC,
        "constraint": StreamFeatureCategory.CONSTRAINT,
    }

    def __init__(
        self,
        model_version: str = "1.0.0",
        n_estimators: int = 100,
        max_depth: int = 8,
        use_tree_explainer: bool = True,
        n_background_samples: int = 100,
        random_state: int = 42,
    ) -> None:
        """
        Initialize SHAP Pinch Explainer.

        Args:
            model_version: Version identifier for the explainer
            n_estimators: Number of trees in surrogate forest
            max_depth: Maximum tree depth
            use_tree_explainer: Use TreeExplainer (faster, exact for trees)
            n_background_samples: Background samples for KernelSHAP fallback
            random_state: Random seed for reproducibility
        """
        self.model_version = model_version
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.use_tree_explainer = use_tree_explainer
        self.n_background_samples = n_background_samples
        self.random_state = random_state

        # Surrogate models for each output type
        self._surrogate_models: Dict[PinchOutputType, Any] = {}
        self._scalers: Dict[PinchOutputType, Any] = {}
        self._feature_names: List[str] = []

        # Historical results for global summaries
        self._historical_results: Dict[PinchOutputType, List[SHAPPinchResult]] = {
            output_type: [] for output_type in PinchOutputType
        }

        logger.info(f"Initialized SHAPPinchExplainer v{self.VERSION}")

    def explain_pinch_result(
        self,
        pinch_result: PinchAnalysisResult,
        hot_streams: List[HeatStream],
        cold_streams: List[HeatStream],
        output_type: PinchOutputType = PinchOutputType.HOT_UTILITY,
    ) -> SHAPPinchResult:
        """
        Explain a pinch analysis result using SHAP values.

        Args:
            pinch_result: Result from pinch analysis calculation
            hot_streams: Hot process streams
            cold_streams: Cold process streams
            output_type: Which output to explain

        Returns:
            SHAPPinchResult with feature attributions
        """
        # Extract features
        features, feature_names = self._extract_features(
            hot_streams, cold_streams, pinch_result.delta_t_min_C
        )
        self._feature_names = feature_names

        # Get actual output value
        actual_value = self._get_output_value(pinch_result, output_type)

        # Build or retrieve surrogate model
        if output_type not in self._surrogate_models:
            self._build_surrogate_model(
                features, actual_value, output_type, hot_streams, cold_streams
            )

        # Calculate SHAP values
        shap_values, base_value, predicted_value = self._calculate_shap_values(
            features, output_type
        )

        # Calculate feature importance
        feature_importance = self._calculate_feature_importance(
            shap_values, feature_names
        )

        # Identify top drivers with engineering interpretations
        top_drivers = self._identify_top_drivers(
            shap_values, features, feature_names, actual_value,
            hot_streams, cold_streams, k=10
        )

        # Calculate interaction effects
        interaction_effects = self._calculate_interactions(
            features, feature_names, shap_values
        )

        # Calculate stream-level contributions
        hot_contributions = self._calculate_stream_contributions(
            shap_values, feature_names, hot_streams, "hot"
        )
        cold_contributions = self._calculate_stream_contributions(
            shap_values, feature_names, cold_streams, "cold"
        )

        # Calculate delta T min sensitivity
        delta_t_sensitivity = self._calculate_delta_t_sensitivity(
            features, feature_names, output_type
        )

        # Compute provenance hash
        computation_hash = self._compute_hash(
            features, shap_values, feature_importance, actual_value
        )

        result = SHAPPinchResult(
            output_type=output_type,
            calculation_id=pinch_result.calculation_id,
            feature_names=feature_names,
            shap_values=shap_values,
            base_value=base_value,
            feature_values=features,
            predicted_value=predicted_value,
            actual_value=actual_value,
            feature_importance=feature_importance,
            top_drivers=top_drivers,
            interaction_effects=interaction_effects,
            hot_stream_contributions=hot_contributions,
            cold_stream_contributions=cold_contributions,
            delta_t_min_sensitivity=delta_t_sensitivity,
            computation_hash=computation_hash,
            model_version=self.model_version,
        )

        # Store for global analysis
        self._historical_results[output_type].append(result)

        logger.info(
            f"Explained {output_type.value} for calculation {pinch_result.calculation_id}"
        )

        return result

    def explain_all_outputs(
        self,
        pinch_result: PinchAnalysisResult,
        hot_streams: List[HeatStream],
        cold_streams: List[HeatStream],
    ) -> Dict[PinchOutputType, SHAPPinchResult]:
        """
        Explain all pinch analysis outputs at once.

        Args:
            pinch_result: Pinch analysis result
            hot_streams: Hot process streams
            cold_streams: Cold process streams

        Returns:
            Dictionary mapping output types to their SHAP explanations
        """
        results = {}

        for output_type in [
            PinchOutputType.HOT_UTILITY,
            PinchOutputType.COLD_UTILITY,
            PinchOutputType.PINCH_TEMPERATURE,
            PinchOutputType.HEAT_RECOVERY,
        ]:
            try:
                results[output_type] = self.explain_pinch_result(
                    pinch_result, hot_streams, cold_streams, output_type
                )
            except Exception as e:
                logger.error(f"Failed to explain {output_type.value}: {e}")

        return results

    def generate_global_explanation(
        self,
        output_type: PinchOutputType,
        results: Optional[List[SHAPPinchResult]] = None,
    ) -> GlobalPinchExplanation:
        """
        Generate global explanation across multiple analyses.

        Args:
            output_type: Which output type to summarize
            results: List of SHAP results (uses historical if None)

        Returns:
            GlobalPinchExplanation with aggregated insights
        """
        if results is None:
            results = self._historical_results.get(output_type, [])

        if not results:
            logger.warning(f"No results available for {output_type.value}")
            return self._create_empty_global_explanation(output_type)

        # Filter to matching output type
        results = [r for r in results if r.output_type == output_type]

        if not results:
            return self._create_empty_global_explanation(output_type)

        # Aggregate SHAP values
        all_shap_values = np.array([r.shap_values for r in results])
        all_feature_values = np.array([r.feature_values for r in results])
        feature_names = results[0].feature_names

        # Mean absolute SHAP
        mean_abs_shap = {}
        for i, name in enumerate(feature_names):
            mean_abs_shap[name] = round(
                float(np.mean(np.abs(all_shap_values[:, i]))), 6
            )

        # Feature ranking
        feature_ranking = sorted(
            mean_abs_shap.keys(),
            key=lambda x: mean_abs_shap[x],
            reverse=True
        )

        # Direction consistency
        direction_consistency = {}
        for i, name in enumerate(feature_names):
            values = all_shap_values[:, i]
            positive_ratio = np.mean(values > 0)
            consistency = max(positive_ratio, 1 - positive_ratio)
            direction_consistency[name] = round(float(consistency), 4)

        # Typical ranges
        typical_ranges = {}
        for i, name in enumerate(feature_names):
            values = all_feature_values[:, i]
            typical_ranges[name] = (
                round(float(np.percentile(values, 10)), 2),
                round(float(np.percentile(values, 90)), 2)
            )

        # Sensitivity matrix
        sensitivity_matrix = self._calculate_sensitivity_matrix(results)

        # Compute hash
        computation_hash = self._compute_global_hash(
            mean_abs_shap, feature_ranking, len(results)
        )

        return GlobalPinchExplanation(
            output_type=output_type,
            model_version=self.model_version,
            num_analyses=len(results),
            mean_abs_shap=mean_abs_shap,
            feature_ranking=feature_ranking,
            direction_consistency=direction_consistency,
            typical_ranges=typical_ranges,
            sensitivity_matrix=sensitivity_matrix,
            computation_hash=computation_hash,
        )

    def generate_explainability_report(
        self,
        pinch_result: PinchAnalysisResult,
        hot_streams: List[HeatStream],
        cold_streams: List[HeatStream],
        hen_design: Optional[HENDesign] = None,
    ) -> ExplainabilityReport:
        """
        Generate comprehensive explainability report.

        Args:
            pinch_result: Pinch analysis result
            hot_streams: Hot process streams
            cold_streams: Cold process streams
            hen_design: Optional HEN design for additional context

        Returns:
            ExplainabilityReport with all explanations
        """
        # Get SHAP explanations for key outputs
        shap_results = self.explain_all_outputs(
            pinch_result, hot_streams, cold_streams
        )

        # Build feature attributions
        shap_values_list = []
        if PinchOutputType.HOT_UTILITY in shap_results:
            hot_result = shap_results[PinchOutputType.HOT_UTILITY]
            for i, name in enumerate(hot_result.feature_names):
                shap_values_list.append(FeatureAttribution(
                    feature_name=name,
                    feature_value=float(hot_result.feature_values[i]),
                    attribution=float(hot_result.shap_values[i]),
                    normalized_attribution=float(
                        hot_result.shap_values[i] /
                        (np.sum(np.abs(hot_result.shap_values)) + 1e-10)
                    ),
                ))

        # Build engineering rationale
        constraint_rationale = self._generate_constraint_rationale(
            pinch_result, hot_streams, cold_streams
        )

        pinch_rule_explanations = self._generate_pinch_rules(
            pinch_result, hot_streams, cold_streams
        )

        temperature_feasibility_notes = self._check_temperature_feasibility(
            hot_streams, cold_streams, pinch_result.delta_t_min_C
        )

        # Build sensitivity analysis
        sensitivity_analysis = {}
        for output_type, result in shap_results.items():
            sensitivity_analysis[output_type.value] = {
                driver.feature_name: driver.sensitivity
                for driver in result.top_drivers[:5]
            }

        # Executive summary
        executive_summary = self._generate_executive_summary(
            pinch_result, shap_results, hot_streams, cold_streams
        )

        # Compute provenance
        provenance_hash = hashlib.sha256(
            json.dumps({
                "calculation_id": pinch_result.calculation_id,
                "summary": executive_summary[:100],
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }, sort_keys=True).encode()
        ).hexdigest()

        return ExplainabilityReport(
            optimization_id=pinch_result.calculation_id,
            constraint_rationale=constraint_rationale,
            pinch_rule_explanations=pinch_rule_explanations,
            temperature_feasibility_notes=temperature_feasibility_notes,
            shap_values=shap_values_list,
            sensitivity_analysis=sensitivity_analysis,
            executive_summary=executive_summary,
            provenance_hash=provenance_hash,
        )

    def _extract_features(
        self,
        hot_streams: List[HeatStream],
        cold_streams: List[HeatStream],
        delta_t_min: float,
    ) -> Tuple[np.ndarray, List[str]]:
        """Extract numerical features from streams."""
        features = []
        names = []

        # Hot stream features
        for i, stream in enumerate(hot_streams):
            prefix = f"hot_{i}"
            features.extend([
                stream.T_supply_C,
                stream.T_target_C,
                stream.m_dot_kg_s,
                stream.Cp_kJ_kgK,
                stream.FCp_kW_K,
                stream.duty_kW,
                stream.T_supply_C - stream.T_target_C,  # Temperature range
            ])
            names.extend([
                f"{prefix}_T_supply",
                f"{prefix}_T_target",
                f"{prefix}_m_dot",
                f"{prefix}_Cp",
                f"{prefix}_FCp",
                f"{prefix}_duty",
                f"{prefix}_delta_T",
            ])

        # Cold stream features
        for i, stream in enumerate(cold_streams):
            prefix = f"cold_{i}"
            features.extend([
                stream.T_supply_C,
                stream.T_target_C,
                stream.m_dot_kg_s,
                stream.Cp_kJ_kgK,
                stream.FCp_kW_K,
                stream.duty_kW,
                stream.T_target_C - stream.T_supply_C,  # Temperature range
            ])
            names.extend([
                f"{prefix}_T_supply",
                f"{prefix}_T_target",
                f"{prefix}_m_dot",
                f"{prefix}_Cp",
                f"{prefix}_FCp",
                f"{prefix}_duty",
                f"{prefix}_delta_T",
            ])

        # System-level features
        total_hot_duty = sum(s.duty_kW for s in hot_streams)
        total_cold_duty = sum(s.duty_kW for s in cold_streams)
        max_hot_temp = max(s.T_supply_C for s in hot_streams)
        min_cold_temp = min(s.T_supply_C for s in cold_streams)

        features.extend([
            delta_t_min,
            total_hot_duty,
            total_cold_duty,
            total_hot_duty - total_cold_duty,  # Duty imbalance
            max_hot_temp,
            min_cold_temp,
            max_hot_temp - min_cold_temp,  # Temperature span
            len(hot_streams),
            len(cold_streams),
        ])
        names.extend([
            "delta_t_min",
            "total_hot_duty",
            "total_cold_duty",
            "duty_imbalance",
            "max_hot_temp",
            "min_cold_temp",
            "temp_span",
            "n_hot_streams",
            "n_cold_streams",
        ])

        return np.array(features), names

    def _get_output_value(
        self,
        pinch_result: PinchAnalysisResult,
        output_type: PinchOutputType,
    ) -> float:
        """Get the output value for a given output type."""
        output_map = {
            PinchOutputType.HOT_UTILITY: pinch_result.minimum_hot_utility_kW,
            PinchOutputType.COLD_UTILITY: pinch_result.minimum_cold_utility_kW,
            PinchOutputType.PINCH_TEMPERATURE: pinch_result.pinch_temperature_C,
            PinchOutputType.HEAT_RECOVERY: pinch_result.maximum_heat_recovery_kW,
        }
        return output_map.get(output_type, 0.0)

    def _build_surrogate_model(
        self,
        features: np.ndarray,
        actual_value: float,
        output_type: PinchOutputType,
        hot_streams: List[HeatStream],
        cold_streams: List[HeatStream],
    ) -> None:
        """Build surrogate model for SHAP analysis."""
        if not HAS_SKLEARN:
            logger.warning("sklearn not available, using simplified model")
            self._surrogate_models[output_type] = None
            return

        n_samples = 200
        n_features = len(features)

        # Generate synthetic training data with physics-informed variations
        np.random.seed(self.random_state)
        X_train = np.zeros((n_samples, n_features))
        y_train = np.zeros(n_samples)

        for i in range(n_samples):
            # Perturb features
            perturbation = np.random.randn(n_features) * 0.15
            X_train[i] = features * (1 + perturbation)
            X_train[i] = np.maximum(X_train[i], 0.01)  # Keep positive

            # Generate physics-based target
            # This is a simplified model of pinch analysis relationships
            if output_type == PinchOutputType.HOT_UTILITY:
                # Hot utility increases with cold stream duty imbalance
                cold_duty_idx = [j for j, n in enumerate(self._feature_names) if "cold" in n and "duty" in n]
                hot_duty_idx = [j for j, n in enumerate(self._feature_names) if "hot" in n and "duty" in n]
                cold_duty = sum(X_train[i, j] for j in cold_duty_idx)
                hot_duty = sum(X_train[i, j] for j in hot_duty_idx)
                y_train[i] = max(0, cold_duty - hot_duty) + np.random.randn() * 10

            elif output_type == PinchOutputType.COLD_UTILITY:
                # Cold utility increases with hot stream duty imbalance
                cold_duty_idx = [j for j, n in enumerate(self._feature_names) if "cold" in n and "duty" in n]
                hot_duty_idx = [j for j, n in enumerate(self._feature_names) if "hot" in n and "duty" in n]
                cold_duty = sum(X_train[i, j] for j in cold_duty_idx)
                hot_duty = sum(X_train[i, j] for j in hot_duty_idx)
                y_train[i] = max(0, hot_duty - cold_duty) + np.random.randn() * 10

            elif output_type == PinchOutputType.PINCH_TEMPERATURE:
                # Pinch temperature is between hot and cold streams
                hot_T_idx = [j for j, n in enumerate(self._feature_names) if "hot" in n and "T_target" in n]
                cold_T_idx = [j for j, n in enumerate(self._feature_names) if "cold" in n and "T_target" in n]
                avg_hot = np.mean([X_train[i, j] for j in hot_T_idx]) if hot_T_idx else 100
                avg_cold = np.mean([X_train[i, j] for j in cold_T_idx]) if cold_T_idx else 50
                y_train[i] = (avg_hot + avg_cold) / 2 + np.random.randn() * 5

            elif output_type == PinchOutputType.HEAT_RECOVERY:
                # Recovery is limited by smaller duty
                cold_duty_idx = [j for j, n in enumerate(self._feature_names) if "cold" in n and "duty" in n]
                hot_duty_idx = [j for j, n in enumerate(self._feature_names) if "hot" in n and "duty" in n]
                cold_duty = sum(X_train[i, j] for j in cold_duty_idx)
                hot_duty = sum(X_train[i, j] for j in hot_duty_idx)
                y_train[i] = min(hot_duty, cold_duty) * 0.9 + np.random.randn() * 10

        # Scale features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X_train)
        self._scalers[output_type] = scaler

        # Train surrogate model
        model = GradientBoostingRegressor(
            n_estimators=self.n_estimators,
            max_depth=self.max_depth,
            random_state=self.random_state,
        )
        model.fit(X_scaled, y_train)
        self._surrogate_models[output_type] = model

        logger.info(f"Built surrogate model for {output_type.value}")

    def _calculate_shap_values(
        self,
        features: np.ndarray,
        output_type: PinchOutputType,
    ) -> Tuple[np.ndarray, float, float]:
        """Calculate SHAP values using TreeExplainer."""
        model = self._surrogate_models.get(output_type)
        scaler = self._scalers.get(output_type)

        if model is None:
            return np.zeros(len(features)), 0.0, 0.0

        # Scale features
        features_scaled = scaler.transform(features.reshape(1, -1))
        predicted_value = float(model.predict(features_scaled)[0])

        try:
            if HAS_SHAP and self.use_tree_explainer:
                explainer = shap.TreeExplainer(model)
                shap_values = explainer(features_scaled)
                return (
                    shap_values.values[0],
                    float(shap_values.base_values[0]),
                    predicted_value
                )
            else:
                # Fallback to permutation importance
                return self._calculate_permutation_importance(
                    features_scaled[0], model, predicted_value
                )
        except Exception as e:
            logger.error(f"SHAP calculation failed: {e}")
            return self._calculate_permutation_importance(
                features_scaled[0], model, predicted_value
            )

    def _calculate_permutation_importance(
        self,
        features: np.ndarray,
        model: Any,
        base_pred: float,
    ) -> Tuple[np.ndarray, float, float]:
        """Fallback permutation-based importance calculation."""
        importance = np.zeros(len(features))

        for i in range(len(features)):
            perturbed = features.copy()
            perturbed[i] *= 0.9  # 10% perturbation
            new_pred = model.predict(perturbed.reshape(1, -1))[0]
            importance[i] = base_pred - new_pred

        return importance, base_pred, base_pred

    def _calculate_feature_importance(
        self,
        shap_values: np.ndarray,
        feature_names: List[str],
    ) -> Dict[str, float]:
        """Calculate aggregated feature importance."""
        importance = {}
        for i, name in enumerate(feature_names):
            importance[name] = round(float(np.abs(shap_values[i])), 6)

        return dict(sorted(
            importance.items(),
            key=lambda x: x[1],
            reverse=True
        ))

    def _identify_top_drivers(
        self,
        shap_values: np.ndarray,
        features: np.ndarray,
        feature_names: List[str],
        actual_value: float,
        hot_streams: List[HeatStream],
        cold_streams: List[HeatStream],
        k: int = 10,
    ) -> List[PinchDriverInfo]:
        """Identify top drivers with engineering interpretations."""
        top_drivers = []
        total_abs_shap = np.sum(np.abs(shap_values))

        # Sort by absolute SHAP value
        indices = np.argsort(np.abs(shap_values))[::-1]

        for rank, idx in enumerate(indices[:k], 1):
            name = feature_names[idx]
            shap_val = float(shap_values[idx])
            feat_val = float(features[idx])

            # Determine category
            category = self._categorize_feature(name)

            # Direction
            direction = "increases" if shap_val > 0 else "decreases"

            # Contribution percentage
            contribution_pct = (
                abs(shap_val) / total_abs_shap * 100
                if total_abs_shap > 0 else 0.0
            )

            # Stream ID
            stream_id = self._extract_stream_id(name, hot_streams, cold_streams)

            # Engineering interpretation
            interpretation = self._get_engineering_interpretation(
                name, shap_val, feat_val, category
            )

            # Sensitivity (approximate)
            sensitivity = shap_val / (feat_val + 1e-10) if feat_val != 0 else 0.0

            top_drivers.append(PinchDriverInfo(
                feature_name=name,
                shap_value=round(shap_val, 6),
                feature_value=round(feat_val, 4),
                direction=direction,
                category=category,
                rank=rank,
                contribution_percent=round(contribution_pct, 2),
                stream_id=stream_id,
                engineering_interpretation=interpretation,
                sensitivity=round(sensitivity, 6),
            ))

        return top_drivers

    def _calculate_interactions(
        self,
        features: np.ndarray,
        feature_names: List[str],
        shap_values: np.ndarray,
    ) -> Dict[str, Dict[str, float]]:
        """Calculate pairwise feature interactions."""
        interactions: Dict[str, Dict[str, float]] = {}

        # Get top features
        top_indices = np.argsort(np.abs(shap_values))[-8:]

        for i in top_indices:
            name_i = feature_names[i]
            interactions[name_i] = {}

            for j in top_indices:
                if i != j:
                    name_j = feature_names[j]
                    # Approximate interaction
                    interaction = (
                        shap_values[i] * shap_values[j] /
                        (abs(shap_values[i]) + abs(shap_values[j]) + 1e-10)
                    )
                    interactions[name_i][name_j] = round(float(interaction), 6)

        return interactions

    def _calculate_stream_contributions(
        self,
        shap_values: np.ndarray,
        feature_names: List[str],
        streams: List[HeatStream],
        stream_type: str,
    ) -> Dict[str, float]:
        """Calculate total SHAP contribution by stream."""
        contributions = {}

        for i, stream in enumerate(streams):
            prefix = f"{stream_type}_{i}"
            total_contribution = 0.0

            for j, name in enumerate(feature_names):
                if name.startswith(prefix):
                    total_contribution += abs(float(shap_values[j]))

            contributions[stream.stream_id] = round(total_contribution, 4)

        return contributions

    def _calculate_delta_t_sensitivity(
        self,
        features: np.ndarray,
        feature_names: List[str],
        output_type: PinchOutputType,
    ) -> float:
        """Calculate sensitivity of output to delta T min."""
        model = self._surrogate_models.get(output_type)
        scaler = self._scalers.get(output_type)

        if model is None or scaler is None:
            return 0.0

        # Find delta_t_min index
        try:
            dt_idx = feature_names.index("delta_t_min")
        except ValueError:
            return 0.0

        # Perturb delta_t_min
        features_low = features.copy()
        features_high = features.copy()

        delta = features[dt_idx] * 0.1  # 10% perturbation
        features_low[dt_idx] -= delta
        features_high[dt_idx] += delta

        # Scale and predict
        low_scaled = scaler.transform(features_low.reshape(1, -1))
        high_scaled = scaler.transform(features_high.reshape(1, -1))

        pred_low = model.predict(low_scaled)[0]
        pred_high = model.predict(high_scaled)[0]

        # Sensitivity = change in output / change in input
        sensitivity = (pred_high - pred_low) / (2 * delta + 1e-10)

        return round(float(sensitivity), 4)

    def _categorize_feature(self, name: str) -> StreamFeatureCategory:
        """Categorize feature by name pattern."""
        name_lower = name.lower()

        for pattern, category in self.FEATURE_CATEGORIES.items():
            if pattern in name_lower:
                return category

        return StreamFeatureCategory.TEMPERATURE

    def _extract_stream_id(
        self,
        name: str,
        hot_streams: List[HeatStream],
        cold_streams: List[HeatStream],
    ) -> Optional[str]:
        """Extract stream ID from feature name."""
        if name.startswith("hot_"):
            try:
                idx = int(name.split("_")[1])
                if idx < len(hot_streams):
                    return hot_streams[idx].stream_id
            except (ValueError, IndexError):
                pass
        elif name.startswith("cold_"):
            try:
                idx = int(name.split("_")[1])
                if idx < len(cold_streams):
                    return cold_streams[idx].stream_id
            except (ValueError, IndexError):
                pass
        return None

    def _get_engineering_interpretation(
        self,
        name: str,
        shap_value: float,
        feature_value: float,
        category: StreamFeatureCategory,
    ) -> str:
        """Generate engineering interpretation for feature contribution."""
        direction = "increases" if shap_value > 0 else "decreases"

        # Look for specific patterns
        for pattern, interpretation in self.ENGINEERING_INTERPRETATIONS.items():
            if pattern in name.lower():
                return f"{interpretation}; current value ({feature_value:.2f}) {direction} utility requirement"

        # Category-based interpretations
        category_interpretations = {
            StreamFeatureCategory.TEMPERATURE: f"Temperature {direction} heat transfer driving force",
            StreamFeatureCategory.FLOW_RATE: f"Flow rate {direction} heat capacity rate and duty",
            StreamFeatureCategory.HEAT_CAPACITY: f"Heat capacity rate {direction} heat exchange capability",
            StreamFeatureCategory.DUTY: f"Heat duty {direction} utility requirements",
            StreamFeatureCategory.APPROACH_TEMP: f"Approach temperature {direction} exchanger size needs",
            StreamFeatureCategory.ECONOMIC: f"Cost factor {direction} project economics",
            StreamFeatureCategory.CONSTRAINT: f"Constraint {direction} feasible solution space",
        }

        return category_interpretations.get(
            category,
            f"Feature {direction} pinch analysis result"
        )

    def _calculate_sensitivity_matrix(
        self,
        results: List[SHAPPinchResult],
    ) -> Dict[str, Dict[str, float]]:
        """Calculate sensitivity matrix from historical results."""
        matrix: Dict[str, Dict[str, float]] = {}

        for result in results:
            output_key = result.output_type.value
            if output_key not in matrix:
                matrix[output_key] = {}

            for driver in result.top_drivers[:5]:
                if driver.feature_name not in matrix[output_key]:
                    matrix[output_key][driver.feature_name] = []
                matrix[output_key][driver.feature_name].append(driver.sensitivity)

        # Average sensitivities
        for output_key in matrix:
            for feature in matrix[output_key]:
                values = matrix[output_key][feature]
                if isinstance(values, list):
                    matrix[output_key][feature] = round(
                        float(np.mean(values)), 4
                    )

        return matrix

    def _generate_constraint_rationale(
        self,
        pinch_result: PinchAnalysisResult,
        hot_streams: List[HeatStream],
        cold_streams: List[HeatStream],
    ) -> List[str]:
        """Generate constraint rationale explanations."""
        rationale = []

        # Delta T min constraint
        rationale.append(
            f"Minimum approach temperature (Delta T min) set to "
            f"{pinch_result.delta_t_min_C} C per heat exchanger design standards"
        )

        # Energy balance
        total_hot = sum(s.duty_kW for s in hot_streams)
        total_cold = sum(s.duty_kW for s in cold_streams)
        rationale.append(
            f"Energy balance: Hot streams = {total_hot:.1f} kW, "
            f"Cold streams = {total_cold:.1f} kW"
        )

        # Utility requirements
        rationale.append(
            f"Minimum hot utility: {pinch_result.minimum_hot_utility_kW:.1f} kW "
            f"required to satisfy cold stream heating needs above the pinch"
        )
        rationale.append(
            f"Minimum cold utility: {pinch_result.minimum_cold_utility_kW:.1f} kW "
            f"required to satisfy hot stream cooling needs below the pinch"
        )

        return rationale

    def _generate_pinch_rules(
        self,
        pinch_result: PinchAnalysisResult,
        hot_streams: List[HeatStream],
        cold_streams: List[HeatStream],
    ) -> List[str]:
        """Generate pinch rule explanations."""
        rules = []

        pinch_T = pinch_result.pinch_temperature_C

        # Golden rules of pinch
        rules.append(
            f"RULE 1: Do not transfer heat across the pinch ({pinch_T:.1f} C) - "
            f"this would increase utility requirements"
        )
        rules.append(
            "RULE 2: Do not use hot utilities below the pinch - "
            "heating below pinch wastes cold utility"
        )
        rules.append(
            "RULE 3: Do not use cold utilities above the pinch - "
            "cooling above pinch wastes hot utility"
        )

        # Stream placement
        hot_above = sum(1 for s in hot_streams if s.T_target_C > pinch_T)
        cold_above = sum(1 for s in cold_streams if s.T_supply_C > pinch_T)
        rules.append(
            f"Above pinch: {hot_above} hot streams available, "
            f"{cold_above} cold streams require heating"
        )

        return rules

    def _check_temperature_feasibility(
        self,
        hot_streams: List[HeatStream],
        cold_streams: List[HeatStream],
        delta_t_min: float,
    ) -> List[str]:
        """Check temperature feasibility and generate notes."""
        notes = []

        # Check for temperature crosses
        for h in hot_streams:
            for c in cold_streams:
                # Check if match is feasible
                if h.T_supply_C - c.T_target_C < delta_t_min:
                    notes.append(
                        f"Potential infeasibility: Hot stream {h.stream_id} "
                        f"({h.T_supply_C:.1f} C) cannot fully heat "
                        f"cold stream {c.stream_id} to {c.T_target_C:.1f} C "
                        f"with Delta T min = {delta_t_min:.1f} C"
                    )

        # Temperature span analysis
        hot_range = max(s.T_supply_C for s in hot_streams) - min(s.T_target_C for s in hot_streams)
        cold_range = max(s.T_target_C for s in cold_streams) - min(s.T_supply_C for s in cold_streams)

        if hot_range < cold_range:
            notes.append(
                f"Note: Cold stream temperature range ({cold_range:.1f} C) exceeds "
                f"hot stream range ({hot_range:.1f} C) - hot utility likely required"
            )

        return notes

    def _generate_executive_summary(
        self,
        pinch_result: PinchAnalysisResult,
        shap_results: Dict[PinchOutputType, SHAPPinchResult],
        hot_streams: List[HeatStream],
        cold_streams: List[HeatStream],
    ) -> str:
        """Generate executive summary of the analysis."""
        summary_parts = []

        # Header
        summary_parts.append(
            f"PINCH ANALYSIS SUMMARY - Calculation {pinch_result.calculation_id}"
        )

        # Key results
        summary_parts.append(
            f"\nKey Results:"
            f"\n  - Pinch Temperature: {pinch_result.pinch_temperature_C:.1f} C"
            f"\n  - Minimum Hot Utility: {pinch_result.minimum_hot_utility_kW:.1f} kW"
            f"\n  - Minimum Cold Utility: {pinch_result.minimum_cold_utility_kW:.1f} kW"
            f"\n  - Maximum Heat Recovery: {pinch_result.maximum_heat_recovery_kW:.1f} kW"
        )

        # Top drivers
        if PinchOutputType.HOT_UTILITY in shap_results:
            hot_result = shap_results[PinchOutputType.HOT_UTILITY]
            if hot_result.top_drivers:
                top_driver = hot_result.top_drivers[0]
                summary_parts.append(
                    f"\nPrimary Driver of Hot Utility:"
                    f"\n  - {top_driver.feature_name} ({top_driver.contribution_percent:.1f}% contribution)"
                    f"\n  - {top_driver.engineering_interpretation}"
                )

        # Recommendations
        summary_parts.append(
            f"\nRecommendations:"
            f"\n  - Follow the three golden rules of pinch design"
            f"\n  - Consider process modifications above the pinch to reduce hot utility"
            f"\n  - Evaluate heat recovery from exhaust streams"
        )

        return "\n".join(summary_parts)

    def _create_empty_global_explanation(
        self,
        output_type: PinchOutputType,
    ) -> GlobalPinchExplanation:
        """Create empty global explanation when no data available."""
        return GlobalPinchExplanation(
            output_type=output_type,
            model_version=self.model_version,
            num_analyses=0,
            mean_abs_shap={},
            feature_ranking=[],
            direction_consistency={},
            typical_ranges={},
            sensitivity_matrix={},
            computation_hash="",
        )

    def _compute_hash(
        self,
        features: np.ndarray,
        shap_values: np.ndarray,
        importance: Dict[str, float],
        actual_value: float,
    ) -> str:
        """Compute SHA-256 hash for provenance."""
        data = {
            "features": features.tolist(),
            "shap_values": shap_values.tolist(),
            "importance": importance,
            "actual_value": actual_value,
            "model_version": self.model_version,
            "explainer_version": self.VERSION,
        }
        data_str = json.dumps(data, sort_keys=True)
        return hashlib.sha256(data_str.encode()).hexdigest()

    def _compute_global_hash(
        self,
        mean_abs_shap: Dict[str, float],
        feature_ranking: List[str],
        num_analyses: int,
    ) -> str:
        """Compute SHA-256 hash for global summary."""
        data = {
            "mean_abs_shap": mean_abs_shap,
            "feature_ranking": feature_ranking,
            "num_analyses": num_analyses,
            "model_version": self.model_version,
            "explainer_version": self.VERSION,
        }
        data_str = json.dumps(data, sort_keys=True)
        return hashlib.sha256(data_str.encode()).hexdigest()
