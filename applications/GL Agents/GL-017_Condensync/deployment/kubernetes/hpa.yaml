# GL-017_Condensync Horizontal Pod Autoscaler
# Auto-scaling configuration for Condenser Optimization Agent
# Version: 1.0.0

---
# Horizontal Pod Autoscaler v2
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: gl-017-condensync-hpa
  namespace: greenlang
  labels:
    app: gl-017-condensync
    app.kubernetes.io/name: gl-017-condensync
    app.kubernetes.io/component: autoscaler
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: gl-017-condensync

  # Replica Limits
  minReplicas: 3
  maxReplicas: 20

  # Scaling Metrics
  metrics:
    # CPU Utilization
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70

    # Memory Utilization
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80

    # Custom Metrics (Prometheus) - HTTP Requests per Second
    # Requires prometheus-adapter
    - type: Pods
      pods:
        metric:
          name: http_requests_per_second
        target:
          type: AverageValue
          averageValue: 100m  # 100 millicore = 0.1 request per second per pod

    # Custom Metrics - Optimization Queue Length
    - type: Pods
      pods:
        metric:
          name: optimization_queue_length
        target:
          type: AverageValue
          averageValue: 10

    # External Metrics (e.g., RabbitMQ queue length)
    # Requires external-metrics-server
    # - type: External
    #   external:
    #     metric:
    #       name: rabbitmq_queue_messages
    #       selector:
    #         matchLabels:
    #           queue: gl-017-condensync-tasks
    #     target:
    #       type: AverageValue
    #       averageValue: 50

  # Scaling Behavior
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        # Scale up by 4 pods maximum per 60 seconds
        - type: Pods
          value: 4
          periodSeconds: 60
        # Or scale up by 100% per 60 seconds
        - type: Percent
          value: 100
          periodSeconds: 60
      selectPolicy: Max

    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        # Scale down by 2 pods maximum per 120 seconds
        - type: Pods
          value: 2
          periodSeconds: 120
        # Or scale down by 25% per 120 seconds
        - type: Percent
          value: 25
          periodSeconds: 120
      selectPolicy: Min

---
# Vertical Pod Autoscaler (VPA)
# Requires VPA controller to be installed
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: gl-017-condensync-vpa
  namespace: greenlang
  labels:
    app: gl-017-condensync
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: gl-017-condensync
  updatePolicy:
    updateMode: "Auto"  # Options: Off, Initial, Recreate, Auto
  resourcePolicy:
    containerPolicies:
      - containerName: gl-017-condensync
        minAllowed:
          cpu: 250m
          memory: 256Mi
        maxAllowed:
          cpu: 4
          memory: 8Gi
        controlledResources:
          - cpu
          - memory
        controlledValues: RequestsAndLimits

---
# KEDA ScaledObject (Alternative/Additional Scaling)
# Requires KEDA operator to be installed
# apiVersion: keda.sh/v1alpha1
# kind: ScaledObject
# metadata:
#   name: gl-017-condensync-keda
#   namespace: greenlang
#   labels:
#     app: gl-017-condensync
# spec:
#   scaleTargetRef:
#     apiVersion: apps/v1
#     kind: Deployment
#     name: gl-017-condensync
#   pollingInterval: 30
#   cooldownPeriod: 300
#   minReplicaCount: 3
#   maxReplicaCount: 50
#   advanced:
#     horizontalPodAutoscalerConfig:
#       behavior:
#         scaleDown:
#           stabilizationWindowSeconds: 300
#           policies:
#             - type: Percent
#               value: 25
#               periodSeconds: 120
#   triggers:
#     # Prometheus Trigger - Scale based on optimization request rate
#     - type: prometheus
#       metadata:
#         serverAddress: http://prometheus-server.monitoring.svc.cluster.local:9090
#         metricName: optimization_requests_total
#         threshold: "100"
#         query: sum(rate(http_requests_total{app="gl-017-condensync",path="/api/v1/optimize"}[2m]))
#
#     # Redis Queue Trigger - Scale based on pending optimizations
#     - type: redis
#       metadata:
#         address: redis.cache.svc.cluster.local:6379
#         listName: gl-017-condensync-optimization-queue
#         listLength: "50"
#
#     # RabbitMQ Trigger - Scale based on message queue
#     - type: rabbitmq
#       metadata:
#         host: amqp://rabbitmq.messaging.svc.cluster.local:5672
#         queueName: gl-017-condensync-tasks
#         queueLength: "100"
#
#     # CPU Trigger
#     - type: cpu
#       metricType: Utilization
#       metadata:
#         value: "70"
#
#     # Memory Trigger
#     - type: memory
#       metricType: Utilization
#       metadata:
#         value: "80"

---
# Pod Autoscaler Metrics Configuration
# ConfigMap for custom metrics configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: gl-017-condensync-autoscaler-config
  namespace: greenlang
  labels:
    app: gl-017-condensync
data:
  # Prometheus Adapter rules for custom metrics
  custom-metrics-rules.yaml: |
    rules:
      # HTTP requests per second
      - seriesQuery: 'http_requests_total{namespace="greenlang",app="gl-017-condensync"}'
        resources:
          overrides:
            namespace: {resource: "namespace"}
            pod: {resource: "pod"}
        name:
          matches: "^(.*)_total$"
          as: "${1}_per_second"
        metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>}[2m])) by (<<.GroupBy>>)'

      # Request duration p95
      - seriesQuery: 'http_request_duration_seconds_bucket{namespace="greenlang",app="gl-017-condensync"}'
        resources:
          overrides:
            namespace: {resource: "namespace"}
            pod: {resource: "pod"}
        name:
          matches: "^(.*)_bucket$"
          as: "${1}_p95"
        metricsQuery: 'histogram_quantile(0.95, sum(rate(<<.Series>>{<<.LabelMatchers>>}[5m])) by (le, <<.GroupBy>>))'

      # Optimization queue length
      - seriesQuery: 'condensync_optimization_queue_length{namespace="greenlang",app="gl-017-condensync"}'
        resources:
          overrides:
            namespace: {resource: "namespace"}
            pod: {resource: "pod"}
        name:
          as: "optimization_queue_length"
        metricsQuery: 'avg(<<.Series>>{<<.LabelMatchers>>}) by (<<.GroupBy>>)'

      # Active optimization jobs
      - seriesQuery: 'condensync_active_optimizations{namespace="greenlang",app="gl-017-condensync"}'
        resources:
          overrides:
            namespace: {resource: "namespace"}
            pod: {resource: "pod"}
        name:
          as: "active_optimizations"
        metricsQuery: 'sum(<<.Series>>{<<.LabelMatchers>>}) by (<<.GroupBy>>)'
