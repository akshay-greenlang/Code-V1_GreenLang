[pytest]
################################################################################
# GL-VCCI Scope 3 Platform - Centralized Pytest Configuration
# Test Suite: 357+ tests across 12 calculator category files
# Target Coverage: 90%+
################################################################################
#
# Purpose: Comprehensive test execution configuration for VCCI Scope 3 Carbon
#          Accounting Platform covering Categories 2-15
#
# Usage:
#   pytest                                    # Run all tests
#   pytest -m unit                           # Run unit tests only
#   pytest -m integration                    # Run integration tests
#   pytest -m calculator                     # Run calculator tests
#   pytest tests/agents/calculator/          # Run all calculator tests
#   pytest tests/agents/calculator/test_category_11.py  # Specific category
#   pytest --cov                             # Run with coverage
#
# Version: 1.0.0
# Created: 2025-11-08
################################################################################

# ----------------------------------------------------------------------------
# Test Discovery
# ----------------------------------------------------------------------------
testpaths =
    tests/agents/calculator
    tests/e2e
    tests/load
    connectors/tests
    entity_mdm/ml/tests
    utils/ml/tests

python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Minimum Python version required
minversion = 7.4

# ----------------------------------------------------------------------------
# Pytest Options
# ----------------------------------------------------------------------------
addopts =
    # Verbosity and output
    -v
    --strict-markers
    --strict-config
    --tb=short
    --color=yes

    # Show test durations (slowest 10 tests)
    --durations=10
    --durations-min=0.5

    # Show local variables in tracebacks
    -l

    # Show summary of all test outcomes
    -ra

    # Fail fast options
    --maxfail=10

    # Capture settings
    --capture=no
    --show-capture=all

# Asyncio mode (required for async tests)
asyncio_mode = auto

# Console output style
console_output_style = progress

# ----------------------------------------------------------------------------
# Test Markers - Comprehensive Category and Type Classification
# ----------------------------------------------------------------------------
markers =
    # Test Types
    unit: Unit tests (fast, isolated, single component)
    integration: Integration tests (multiple components working together)
    e2e: End-to-end workflow tests (complete pipeline execution)
    load: Load and performance tests

    # Priority Levels
    critical: Critical tests - MUST PASS (zero hallucination, compliance)
    high: High priority tests (core functionality)
    medium: Medium priority tests (standard features)
    low: Low priority tests (nice-to-have features)

    # Performance & Speed
    slow: Slow tests (>1 second execution time)
    fast: Fast tests (<0.1 second execution time)
    performance: Performance benchmarking tests

    # Calculator Categories (Scope 3 Categories 2-15)
    calculator: Calculator Agent tests (all categories)
    category_2: Category 2 - Capital Goods
    category_3: Category 3 - Fuel and Energy Related Activities
    category_5: Category 5 - Waste Generated in Operations
    category_7: Category 7 - Employee Commuting
    category_8: Category 8 - Upstream Leased Assets
    category_9: Category 9 - Downstream Transportation and Distribution
    category_10: Category 10 - Processing of Sold Products
    category_11: Category 11 - Use of Sold Products
    category_12: Category 12 - End-of-Life Treatment of Sold Products
    category_13: Category 13 - Downstream Leased Assets
    category_14: Category 14 - Franchises
    category_15: Category 15 - Investments

    # Tier-Based Tests
    tier_1: Tier 1 calculations (highest quality data)
    tier_2: Tier 2 calculations (medium quality data)
    tier_3: Tier 3 calculations (lowest quality data/LLM-based)

    # Agent Components
    intake: Intake Agent tests
    engagement: Engagement Agent tests
    aggregator: Aggregator Agent tests

    # Infrastructure
    cli: CLI Interface tests
    api: API endpoint tests
    database: Database operation tests

    # Data Quality & Validation
    validation: Validation tests
    data_quality: Data quality scoring tests

    # Special Test Types
    regression: Regression tests (prevent bugs from returning)
    smoke: Smoke tests (basic functionality check)

    # LLM Integration
    llm: LLM-based tests (Claude integration)

    # Connectors
    workday: Workday connector tests
    sap: SAP connector tests
    oracle: Oracle connector tests

# ----------------------------------------------------------------------------
# Coverage Configuration
# ----------------------------------------------------------------------------
[coverage:run]
source =
    services/agents/calculator
    services/agents/intake
    services/agents/engagement
    connectors
    cli
    backend
    entity_mdm
    utils

branch = True
parallel = True
concurrency = multiprocessing

omit =
    */tests/*
    */test_*.py
    */__pycache__/*
    */venv/*
    */virtualenv/*
    */.venv/*
    */site-packages/*
    setup.py
    */migrations/*
    */examples/*
    */scripts/*

[coverage:report]
precision = 2
show_missing = True
skip_covered = False
skip_empty = False

# Fail if coverage is below threshold
fail_under = 85

# Exclude specific patterns from coverage
exclude_lines =
    # Standard pragma
    pragma: no cover

    # Don't complain about missing debug code
    def __repr__
    def __str__

    # Don't complain if tests don't hit defensive assertion code
    raise AssertionError
    raise NotImplementedError

    # Don't complain if non-runnable code isn't run
    if 0:
    if False:
    if __name__ == .__main__.:

    # Don't complain about type checking code
    if TYPE_CHECKING:
    if typing.TYPE_CHECKING:

    # Don't complain about abstract methods
    @abstractmethod
    @abc.abstractmethod

[coverage:html]
directory = htmlcov
title = GL-VCCI Scope 3 Platform - Test Coverage Report

[coverage:json]
output = test-reports/coverage.json
pretty_print = True

# ----------------------------------------------------------------------------
# Logging Configuration
# ----------------------------------------------------------------------------
log_cli = False
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = test-reports/pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# ----------------------------------------------------------------------------
# Warning Filters
# ----------------------------------------------------------------------------
filterwarnings =
    # Ignore deprecation warnings
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning

    # Ignore specific library warnings
    ignore::UserWarning
    ignore::FutureWarning:pandas

    # Convert some warnings to errors (strict mode for important warnings)
    error::RuntimeWarning

    # Ignore cryptography warnings
    ignore:.*cryptography.*:DeprecationWarning

# ----------------------------------------------------------------------------
# Pytest Plugins Configuration
# ----------------------------------------------------------------------------

# Timeout for individual tests (requires pytest-timeout)
timeout = 300
timeout_method = thread

# JUnit XML output (for CI/CD integration)
junit_family = xunit2
junit_suite_name = GL-VCCI Scope 3 Platform Test Suite

# ----------------------------------------------------------------------------
# Quick Reference - Common Test Commands
# ----------------------------------------------------------------------------
#
# Run all calculator tests:
#   pytest tests/agents/calculator/ -v
#
# Run specific category:
#   pytest -m category_11 -v
#
# Run with coverage:
#   pytest --cov --cov-report=html --cov-report=term
#
# Run critical tests only:
#   pytest -m critical -v
#
# Run fast tests (skip slow):
#   pytest -m "not slow" -v
#
# Run specific tier tests:
#   pytest -m tier_1 -v
#
# Run in parallel (requires pytest-xdist):
#   pytest -n auto
#
# Run with detailed output:
#   pytest -vv --tb=long
#
# Generate HTML report:
#   pytest --html=test-reports/report.html --self-contained-html
#
# Run only failed tests from last run:
#   pytest --lf
#
# Run all tests with full reporting:
#   pytest --cov --cov-report=html --cov-report=term \
#          --html=test-reports/report.html --self-contained-html \
#          --junitxml=test-reports/junit.xml
#
# ----------------------------------------------------------------------------
# Expected Test Statistics
# ----------------------------------------------------------------------------
# Total Tests: 357+
# Calculator Tests: 357 (Categories 2-15)
# Target Pass Rate: 90-95%
# Target Coverage: 90%+
# Expected Execution Time: 2-5 minutes (without parallel execution)
# Expected Execution Time: <1 minute (with parallel execution via -n auto)
# ----------------------------------------------------------------------------
