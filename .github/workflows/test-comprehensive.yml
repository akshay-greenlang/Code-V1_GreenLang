# GreenLang Comprehensive Test Suite CI/CD Pipeline
# Author: GL-TestEngineer
# Version: 1.0.0
#
# This workflow runs the complete test suite including:
# - Unit tests (325+ tests)
# - Golden tests (792+ tests)
# - Integration tests (60+ tests)
# - Performance tests (40+ tests)
#
# Targets:
# - 85%+ code coverage
# - All golden tests pass (zero-hallucination validation)
# - Performance targets met

name: Comprehensive Test Suite

on:
  push:
    branches:
      - master
      - main
      - develop
      - 'feature/**'
      - 'release/**'
    paths:
      - 'core/**'
      - 'generated/**'
      - 'tests/**'
      - 'greenlang/**'
      - 'pytest.ini'
      - 'pyproject.toml'
      - 'requirements*.txt'
  pull_request:
    branches:
      - master
      - main
      - develop
    paths:
      - 'core/**'
      - 'generated/**'
      - 'tests/**'
      - 'greenlang/**'
      - 'pytest.ini'
      - 'pyproject.toml'
      - 'requirements*.txt'
  schedule:
    # Run daily at 6 AM UTC
    - cron: '0 6 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Test type to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - golden
          - integration
          - performance
      coverage_threshold:
        description: 'Minimum coverage threshold (%)'
        required: false
        default: '85'

env:
  PYTHON_VERSION: '3.11'
  COVERAGE_THRESHOLD: ${{ github.event.inputs.coverage_threshold || '85' }}

concurrency:
  group: test-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # ==========================================================================
  # Quick Smoke Tests
  # ==========================================================================
  smoke-tests:
    name: Smoke Tests
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-asyncio

      - name: Run smoke tests
        run: |
          pytest tests/ -m "smoke" -v --tb=short -x --maxfail=1 || true

  # ==========================================================================
  # Unit Tests
  # ==========================================================================
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: smoke-tests
    if: ${{ github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'unit' || github.event.inputs.test_type == '' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r tests/requirements_test.txt || pip install pytest pytest-asyncio pytest-cov pytest-xdist

      - name: Run unit tests
        run: |
          pytest tests/unit/ -v \
            --tb=short \
            -m "unit" \
            --cov=core \
            --cov=generated \
            --cov-report=xml:coverage-unit.xml \
            --cov-report=term-missing \
            --junitxml=junit-unit.xml \
            -n auto

      - name: Upload unit test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-results
          path: |
            junit-unit.xml
            coverage-unit.xml
          retention-days: 30

  # ==========================================================================
  # Golden Tests (Zero-Hallucination Validation)
  # ==========================================================================
  golden-tests:
    name: Golden Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: smoke-tests
    if: ${{ github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'golden' || github.event.inputs.test_type == '' }}

    strategy:
      fail-fast: false
      matrix:
        test-group:
          - fuel_emissions
          - cbam
          - building_energy
          - eudr

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r tests/requirements_test.txt || pip install pytest pytest-asyncio

      - name: Run golden tests - ${{ matrix.test-group }}
        run: |
          pytest tests/golden/test_${{ matrix.test-group }}_golden.py -v \
            --tb=short \
            -m "golden" \
            --junitxml=junit-golden-${{ matrix.test-group }}.xml

      - name: Upload golden test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: golden-test-results-${{ matrix.test-group }}
          path: junit-golden-${{ matrix.test-group }}.xml
          retention-days: 30

  # ==========================================================================
  # Integration Tests
  # ==========================================================================
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [unit-tests, golden-tests]
    if: ${{ github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'integration' || github.event.inputs.test_type == '' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r tests/requirements_test.txt || pip install pytest pytest-asyncio pytest-cov

      - name: Run integration tests
        run: |
          pytest tests/integration/ -v \
            --tb=short \
            -m "integration" \
            --cov=core \
            --cov=generated \
            --cov-report=xml:coverage-integration.xml \
            --junitxml=junit-integration.xml

      - name: Upload integration test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results
          path: |
            junit-integration.xml
            coverage-integration.xml
          retention-days: 30

  # ==========================================================================
  # Performance Tests
  # ==========================================================================
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [unit-tests, golden-tests]
    if: ${{ github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'performance' || github.event.inputs.test_type == '' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r tests/requirements_test.txt || pip install pytest pytest-asyncio pytest-benchmark

      - name: Run performance tests
        run: |
          pytest tests/performance/ -v \
            --tb=short \
            -m "performance" \
            --junitxml=junit-performance.xml

      - name: Upload performance test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-test-results
          path: junit-performance.xml
          retention-days: 30

  # ==========================================================================
  # Coverage Aggregation
  # ==========================================================================
  coverage-report:
    name: Coverage Report
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download unit test coverage
        uses: actions/download-artifact@v4
        with:
          name: unit-test-results
          path: coverage/

      - name: Download integration test coverage
        uses: actions/download-artifact@v4
        with:
          name: integration-test-results
          path: coverage/
        continue-on-error: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install coverage tool
        run: pip install coverage

      - name: Combine coverage reports
        run: |
          cd coverage
          coverage combine coverage-*.xml || echo "Single coverage file"
          coverage report --fail-under=${{ env.COVERAGE_THRESHOLD }} || true
          coverage html -d htmlcov

      - name: Upload combined coverage
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: coverage/htmlcov/
          retention-days: 30

      - name: Comment coverage on PR
        if: github.event_name == 'pull_request'
        uses: py-cov-action/python-coverage-comment-action@v3
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          MINIMUM_GREEN: 85
          MINIMUM_ORANGE: 70

  # ==========================================================================
  # Test Summary
  # ==========================================================================
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, golden-tests, integration-tests, performance-tests, coverage-report]
    if: always()

    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: test-results/

      - name: Generate summary
        run: |
          echo "# Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Execution" >> $GITHUB_STEP_SUMMARY
          echo "| Test Type | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Unit Tests | ${{ needs.unit-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Golden Tests | ${{ needs.golden-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Integration Tests | ${{ needs.integration-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Performance Tests | ${{ needs.performance-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Coverage" >> $GITHUB_STEP_SUMMARY
          echo "Target: ${{ env.COVERAGE_THRESHOLD }}%" >> $GITHUB_STEP_SUMMARY

      - name: Check overall status
        if: contains(needs.*.result, 'failure')
        run: |
          echo "One or more test jobs failed"
          exit 1
